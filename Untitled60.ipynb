{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Real-Time Streaming Text-to-Speech System - PART 1\n",
        "# Foundation, Data Loading, and Text Processing\n",
        "# Built from scratch for LibriSpeech train-clean-100\n",
        "\n",
        "# ==============================================================================\n",
        "# CELL 1: Installation and Imports\n",
        "# ==============================================================================\n",
        "\n",
        "\"\"\"\n",
        "Installation commands (run if needed):\n",
        "!pip install torch torchaudio librosa soundfile numpy matplotlib jupyter ipython tqdm\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import librosa\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "import threading\n",
        "import queue\n",
        "import time\n",
        "from typing import List, Tuple, Optional, Generator\n",
        "import soundfile as sf\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "from IPython.display import Audio, display, HTML\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üéâ REAL-TIME STREAMING TTS SYSTEM - PART 1\")\n",
        "print(\"   Foundation, Data Loading, and Text Processing\")\n",
        "print(\"=\" * 60)\n",
        "print(\"‚úÖ All imports successful\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "# ==============================================================================\n",
        "# CELL 2: System Configuration\n",
        "# ==============================================================================\n",
        "\n",
        "class TTSConfig:\n",
        "    \"\"\"Complete configuration for LibriSpeech TTS system\"\"\"\n",
        "\n",
        "    # === DATASET CONFIGURATION ===\n",
        "    LIBRISPEECH_PATH = \"/content/drive/MyDrive/NAC/train-clean-100\"  # UPDATE THIS PATH!\n",
        "    MAX_SAMPLES = None  # None for full dataset, number for subset (e.g., 5000 for testing)\n",
        "    MAX_AUDIO_LENGTH = 15.0  # seconds\n",
        "    MIN_AUDIO_LENGTH = 1.0   # seconds\n",
        "    TRAIN_RATIO = 0.95  # 95% train, 5% validation\n",
        "\n",
        "    # === AUDIO PARAMETERS ===\n",
        "    SAMPLE_RATE = 16000  # LibriSpeech native sample rate\n",
        "    HOP_LENGTH = 200     # Optimized for 16kHz\n",
        "    N_MELS = 80\n",
        "    N_FFT = 1024\n",
        "    WIN_LENGTH = 800\n",
        "    F_MIN = 0\n",
        "    F_MAX = 8000  # Nyquist frequency\n",
        "\n",
        "    # === MODEL ARCHITECTURE ===\n",
        "    VOCAB_SIZE = 2000  # Will be updated after vocabulary building\n",
        "    HIDDEN_DIM = 512\n",
        "    NUM_HEADS = 8\n",
        "    NUM_LAYERS = 6\n",
        "    MEL_DIM = 80\n",
        "    MAX_SEQ_LEN = 500  # Maximum text sequence length\n",
        "\n",
        "    # === TRAINING PARAMETERS ===\n",
        "    BATCH_SIZE = 8  # Adjust based on GPU memory\n",
        "    LEARNING_RATE = 1e-4\n",
        "    NUM_EPOCHS = 100\n",
        "    WARMUP_STEPS = 4000\n",
        "    GRAD_CLIP_NORM = 1.0\n",
        "\n",
        "    # === STREAMING PARAMETERS ===\n",
        "    BUFFER_SIZE = 4      # Number of words to buffer before processing\n",
        "    OVERLAP_FRAMES = 2   # Overlap between chunks for smooth streaming\n",
        "\n",
        "    # === DEVICE CONFIGURATION ===\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    USE_MIXED_PRECISION = torch.cuda.is_available()\n",
        "\n",
        "    # === LOSS WEIGHTS ===\n",
        "    MEL_LOSS_WEIGHT = 1.0\n",
        "    DURATION_LOSS_WEIGHT = 0.1\n",
        "    ALIGNMENT_LOSS_WEIGHT = 0.05\n",
        "\n",
        "    @classmethod\n",
        "    def print_config(cls):\n",
        "        \"\"\"Print current configuration\"\"\"\n",
        "        print(\"üîß TTS System Configuration:\")\n",
        "        print(f\"   LibriSpeech Path: {cls.LIBRISPEECH_PATH}\")\n",
        "        print(f\"   Sample Rate: {cls.SAMPLE_RATE}Hz\")\n",
        "        print(f\"   Batch Size: {cls.BATCH_SIZE}\")\n",
        "        print(f\"   Hidden Dim: {cls.HIDDEN_DIM}\")\n",
        "        print(f\"   Device: {cls.DEVICE}\")\n",
        "        print(f\"   Mixed Precision: {cls.USE_MIXED_PRECISION}\")\n",
        "        print(f\"   Buffer Size: {cls.BUFFER_SIZE} words\")\n",
        "\n",
        "config = TTSConfig()\n",
        "config.print_config()\n",
        "\n",
        "# Verify LibriSpeech path\n",
        "librispeech_path = Path(config.LIBRISPEECH_PATH)\n",
        "if librispeech_path.exists():\n",
        "    print(f\"‚úÖ LibriSpeech found at: {librispeech_path}\")\n",
        "    subdirs = [d.name for d in librispeech_path.iterdir() if d.is_dir()][:5]\n",
        "    print(f\"   Sample directories: {subdirs}\")\n",
        "else:\n",
        "    print(f\"‚ùå LibriSpeech not found at: {librispeech_path}\")\n",
        "    print(\"   Please update config.LIBRISPEECH_PATH to your LibriSpeech directory\")\n",
        "    print(\"   You can download it from: https://www.openslr.org/12/\")\n",
        "\n",
        "# ==============================================================================\n",
        "# CELL 3: LibriSpeech Data Discovery and Loading\n",
        "# ==============================================================================\n",
        "\n",
        "class LibriSpeechDatasetLoader:\n",
        "    \"\"\"Comprehensive LibriSpeech dataset loader with robust error handling\"\"\"\n",
        "\n",
        "    def __init__(self, root_path: str):\n",
        "        self.root_path = Path(root_path)\n",
        "        self.audio_files = []\n",
        "        self.transcripts = {}\n",
        "        self.speakers = set()\n",
        "        self.chapters = set()\n",
        "\n",
        "    def discover_files(self):\n",
        "        \"\"\"Discover all LibriSpeech files with progress tracking\"\"\"\n",
        "        print(f\"üîç Discovering LibriSpeech files in {self.root_path}\")\n",
        "\n",
        "        if not self.root_path.exists():\n",
        "            raise FileNotFoundError(f\"LibriSpeech path not found: {self.root_path}\")\n",
        "\n",
        "        # Find all relevant files\n",
        "        print(\"   Scanning for audio and transcript files...\")\n",
        "        flac_files = list(self.root_path.rglob(\"*.flac\"))\n",
        "        trans_files = list(self.root_path.rglob(\"*.trans.txt\"))\n",
        "\n",
        "        print(f\"   Found {len(flac_files)} audio files (.flac)\")\n",
        "        print(f\"   Found {len(trans_files)} transcript files (.trans.txt)\")\n",
        "\n",
        "        if len(flac_files) == 0:\n",
        "            print(\"‚ùå No .flac files found! Check your LibriSpeech path.\")\n",
        "            return\n",
        "\n",
        "        if len(trans_files) == 0:\n",
        "            print(\"‚ùå No .trans.txt files found! Check your LibriSpeech path.\")\n",
        "            return\n",
        "\n",
        "        # Load transcripts\n",
        "        self._load_transcripts(trans_files)\n",
        "\n",
        "        # Match audio files with transcripts\n",
        "        self._match_audio_transcripts(flac_files)\n",
        "\n",
        "        # Generate statistics\n",
        "        self._generate_statistics()\n",
        "\n",
        "        print(f\"‚úÖ Successfully processed {len(self.audio_files)} audio-transcript pairs\")\n",
        "\n",
        "    def _load_transcripts(self, trans_files):\n",
        "        \"\"\"Load all transcript files with error handling\"\"\"\n",
        "        print(\"üìñ Loading transcript files...\")\n",
        "\n",
        "        total_transcripts = 0\n",
        "        failed_files = 0\n",
        "\n",
        "        for trans_file in tqdm(trans_files, desc=\"Loading transcripts\"):\n",
        "            try:\n",
        "                with open(trans_file, 'r', encoding='utf-8') as f:\n",
        "                    for line_num, line in enumerate(f, 1):\n",
        "                        line = line.strip()\n",
        "                        if line:\n",
        "                            parts = line.split(' ', 1)\n",
        "                            if len(parts) == 2:\n",
        "                                file_id, transcript = parts\n",
        "                                # LibriSpeech uses uppercase text\n",
        "                                self.transcripts[file_id] = transcript.upper()\n",
        "                                total_transcripts += 1\n",
        "                            else:\n",
        "                                print(f\"‚ö†Ô∏è Malformed line {line_num} in {trans_file}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                failed_files += 1\n",
        "                print(f\"‚ùå Error reading {trans_file}: {e}\")\n",
        "\n",
        "        print(f\"üìù Loaded {total_transcripts} transcripts from {len(trans_files)} files\")\n",
        "        if failed_files > 0:\n",
        "            print(f\"‚ö†Ô∏è Failed to read {failed_files} transcript files\")\n",
        "\n",
        "    def _match_audio_transcripts(self, flac_files):\n",
        "        \"\"\"Match audio files with their corresponding transcripts\"\"\"\n",
        "        print(\"üîó Matching audio files with transcripts...\")\n",
        "\n",
        "        matched = 0\n",
        "        missing_transcripts = 0\n",
        "\n",
        "        for flac_file in tqdm(flac_files, desc=\"Matching files\"):\n",
        "            file_id = flac_file.stem  # Filename without extension\n",
        "\n",
        "            if file_id in self.transcripts:\n",
        "                # Extract speaker and chapter information\n",
        "                id_parts = file_id.split('-')\n",
        "                if len(id_parts) >= 2:\n",
        "                    speaker_id = id_parts[0]\n",
        "                    chapter_id = id_parts[1]\n",
        "\n",
        "                    self.audio_files.append({\n",
        "                        'audio_path': str(flac_file),\n",
        "                        'transcript': self.transcripts[file_id],\n",
        "                        'file_id': file_id,\n",
        "                        'speaker_id': speaker_id,\n",
        "                        'chapter_id': chapter_id\n",
        "                    })\n",
        "\n",
        "                    self.speakers.add(speaker_id)\n",
        "                    self.chapters.add(chapter_id)\n",
        "                    matched += 1\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è Unexpected file ID format: {file_id}\")\n",
        "            else:\n",
        "                missing_transcripts += 1\n",
        "\n",
        "        print(f\"‚úÖ Matched {matched} audio files with transcripts\")\n",
        "        if missing_transcripts > 0:\n",
        "            print(f\"‚ö†Ô∏è {missing_transcripts} audio files had no matching transcripts\")\n",
        "\n",
        "    def _generate_statistics(self):\n",
        "        \"\"\"Generate dataset statistics\"\"\"\n",
        "        if not self.audio_files:\n",
        "            return\n",
        "\n",
        "        print(f\"\\nüìä Dataset Statistics:\")\n",
        "        print(f\"   Total samples: {len(self.audio_files)}\")\n",
        "        print(f\"   Unique speakers: {len(self.speakers)}\")\n",
        "        print(f\"   Unique chapters: {len(self.chapters)}\")\n",
        "\n",
        "        # Sample transcript lengths\n",
        "        transcript_lengths = [len(sample['transcript']) for sample in self.audio_files[:1000]]\n",
        "        if transcript_lengths:\n",
        "            avg_length = np.mean(transcript_lengths)\n",
        "            print(f\"   Avg transcript length: {avg_length:.1f} characters\")\n",
        "\n",
        "    def get_samples(self, max_samples=None):\n",
        "        \"\"\"Get processed samples with optional limit\"\"\"\n",
        "        samples = self.audio_files[:max_samples] if max_samples else self.audio_files\n",
        "\n",
        "        if max_samples and max_samples < len(self.audio_files):\n",
        "            print(f\"üìã Using subset: {len(samples)} out of {len(self.audio_files)} total samples\")\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def get_speaker_info(self):\n",
        "        \"\"\"Get speaker and chapter information\"\"\"\n",
        "        return {\n",
        "            'speakers': sorted(list(self.speakers)),\n",
        "            'chapters': sorted(list(self.chapters)),\n",
        "            'total_speakers': len(self.speakers),\n",
        "            'total_chapters': len(self.chapters)\n",
        "        }\n",
        "\n",
        "# Load LibriSpeech dataset\n",
        "librispeech_samples = []\n",
        "dataset_info = {}\n",
        "\n",
        "if librispeech_path.exists():\n",
        "    print(\"\\nüöÄ Loading LibriSpeech dataset...\")\n",
        "    try:\n",
        "        loader = LibriSpeechDatasetLoader(config.LIBRISPEECH_PATH)\n",
        "        loader.discover_files()\n",
        "\n",
        "        # Get samples (use subset for testing if specified)\n",
        "        librispeech_samples = loader.get_samples(max_samples=config.MAX_SAMPLES)\n",
        "        dataset_info = loader.get_speaker_info()\n",
        "\n",
        "        # Show sample transcripts\n",
        "        print(f\"\\nüìù Sample transcripts:\")\n",
        "        for i, sample in enumerate(librispeech_samples[:5]):\n",
        "            transcript_preview = sample['transcript'][:80] + \"...\" if len(sample['transcript']) > 80 else sample['transcript']\n",
        "            print(f\"   {i+1}. {sample['file_id']}: '{transcript_preview}'\")\n",
        "            print(f\"      Speaker: {sample['speaker_id']}, Chapter: {sample['chapter_id']}\")\n",
        "\n",
        "        print(f\"\\n‚úÖ LibriSpeech loaded: {len(librispeech_samples)} samples ready\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load LibriSpeech: {e}\")\n",
        "        librispeech_samples = []\n",
        "else:\n",
        "    print(\"‚ùå Cannot load LibriSpeech - path not found\")\n",
        "    print(\"   Please download LibriSpeech train-clean-100 and update the path in config\")\n",
        "\n",
        "# ==============================================================================\n",
        "# CELL 4: Advanced Text Processing System\n",
        "# ==============================================================================\n",
        "\n",
        "class LibriSpeechTextProcessor:\n",
        "    \"\"\"Advanced text processor optimized for LibriSpeech with multiple tokenization strategies\"\"\"\n",
        "\n",
        "    def __init__(self, use_char_level=True):\n",
        "        # Special tokens for sequence modeling\n",
        "        self.special_tokens = {\n",
        "            '<PAD>': 0,    # Padding token\n",
        "            '<UNK>': 1,    # Unknown token\n",
        "            '<SOS>': 2,    # Start of sequence\n",
        "            '<EOS>': 3,    # End of sequence\n",
        "            '<SPACE>': 4   # Space token for char-level\n",
        "        }\n",
        "\n",
        "        self.vocab = self.special_tokens.copy()\n",
        "        self.vocab_size = len(self.special_tokens)\n",
        "        self.id_to_token = {v: k for k, v in self.vocab.items()}\n",
        "\n",
        "        # Tokenization strategy\n",
        "        self.use_char_level = use_char_level\n",
        "\n",
        "        # Statistics\n",
        "        self.vocab_stats = {}\n",
        "\n",
        "    def build_vocabulary(self, samples: List[dict], min_freq: int = 5, max_vocab_size: int = 5000):\n",
        "        \"\"\"Build vocabulary from LibriSpeech samples with frequency filtering\"\"\"\n",
        "        print(f\"üìù Building vocabulary from {len(samples)} samples...\")\n",
        "        print(f\"   Tokenization: {'Character-level' if self.use_char_level else 'Word-level'}\")\n",
        "        print(f\"   Min frequency: {min_freq}\")\n",
        "        print(f\"   Max vocab size: {max_vocab_size}\")\n",
        "\n",
        "        if self.use_char_level:\n",
        "            self._build_char_vocabulary(samples, max_vocab_size)\n",
        "        else:\n",
        "            self._build_word_vocabulary(samples, min_freq, max_vocab_size)\n",
        "\n",
        "        # Update config with actual vocab size\n",
        "        config.VOCAB_SIZE = self.vocab_size\n",
        "\n",
        "        print(f\"‚úÖ Vocabulary built: {self.vocab_size} tokens\")\n",
        "        self._print_vocab_stats()\n",
        "\n",
        "    def _build_char_vocabulary(self, samples, max_vocab_size):\n",
        "        \"\"\"Build character-level vocabulary with frequency analysis\"\"\"\n",
        "        print(\"üî§ Building character-level vocabulary...\")\n",
        "\n",
        "        char_freq = defaultdict(int)\n",
        "        total_chars = 0\n",
        "\n",
        "        # Count character frequencies\n",
        "        for sample in tqdm(samples, desc=\"Analyzing characters\"):\n",
        "            text = sample['transcript']\n",
        "            for char in text:\n",
        "                char_freq[char] += 1\n",
        "                total_chars += 1\n",
        "\n",
        "        # Sort by frequency and add to vocabulary\n",
        "        sorted_chars = sorted(char_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "        added_chars = 0\n",
        "\n",
        "        for char, freq in sorted_chars:\n",
        "            if char.isprintable() and char not in self.vocab:\n",
        "                if self.vocab_size < max_vocab_size:\n",
        "                    self.vocab[char] = self.vocab_size\n",
        "                    self.vocab_size += 1\n",
        "                    added_chars += 1\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "        # Update mappings\n",
        "        self.id_to_token = {v: k for k, v in self.vocab.items()}\n",
        "\n",
        "        # Store statistics\n",
        "        self.vocab_stats = {\n",
        "            'total_chars_seen': len(char_freq),\n",
        "            'total_char_instances': total_chars,\n",
        "            'chars_added': added_chars,\n",
        "            'coverage': added_chars / len(char_freq) if char_freq else 0\n",
        "        }\n",
        "\n",
        "        print(f\"üìä Character analysis:\")\n",
        "        print(f\"   Unique characters found: {len(char_freq)}\")\n",
        "        print(f\"   Characters added to vocab: {added_chars}\")\n",
        "        print(f\"   Coverage: {self.vocab_stats['coverage']:.1%}\")\n",
        "\n",
        "    def _build_word_vocabulary(self, samples, min_freq, max_vocab_size):\n",
        "        \"\"\"Build word-level vocabulary with frequency filtering\"\"\"\n",
        "        print(f\"üìö Building word-level vocabulary...\")\n",
        "\n",
        "        word_freq = defaultdict(int)\n",
        "        total_words = 0\n",
        "\n",
        "        # Count word frequencies\n",
        "        for sample in tqdm(samples, desc=\"Analyzing words\"):\n",
        "            words = self._tokenize_text(sample['transcript'])\n",
        "            for word in words:\n",
        "                word_freq[word] += 1\n",
        "                total_words += 1\n",
        "\n",
        "        # Sort by frequency and add to vocabulary\n",
        "        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "        added_words = 0\n",
        "\n",
        "        for word, freq in sorted_words:\n",
        "            if freq >= min_freq and word not in self.vocab:\n",
        "                if self.vocab_size < max_vocab_size:\n",
        "                    self.vocab[word] = self.vocab_size\n",
        "                    self.vocab_size += 1\n",
        "                    added_words += 1\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "        # Update mappings\n",
        "        self.id_to_token = {v: k for k, v in self.vocab.items()}\n",
        "\n",
        "        # Store statistics\n",
        "        words_above_threshold = sum(1 for w, f in word_freq.items() if f >= min_freq)\n",
        "        self.vocab_stats = {\n",
        "            'total_words_seen': len(word_freq),\n",
        "            'total_word_instances': total_words,\n",
        "            'words_above_threshold': words_above_threshold,\n",
        "            'words_added': added_words,\n",
        "            'coverage': added_words / words_above_threshold if words_above_threshold else 0\n",
        "        }\n",
        "\n",
        "        print(f\"üìä Word analysis:\")\n",
        "        print(f\"   Unique words found: {len(word_freq)}\")\n",
        "        print(f\"   Words above threshold ({min_freq}): {words_above_threshold}\")\n",
        "        print(f\"   Words added to vocab: {added_words}\")\n",
        "        print(f\"   Coverage: {self.vocab_stats['coverage']:.1%}\")\n",
        "\n",
        "    def _tokenize_text(self, text: str) -> List[str]:\n",
        "        \"\"\"Tokenize text into words with LibriSpeech-specific preprocessing\"\"\"\n",
        "        # LibriSpeech preprocessing: keep letters, numbers, apostrophes, hyphens\n",
        "        text = re.sub(r'[^\\w\\s\\'\\-]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text.strip())\n",
        "        return text.split()\n",
        "\n",
        "    def text_to_ids(self, text: str) -> List[int]:\n",
        "        \"\"\"Convert text to token IDs with proper sequence markers\"\"\"\n",
        "        if self.use_char_level:\n",
        "            # Character-level encoding\n",
        "            ids = []\n",
        "            for char in text:\n",
        "                token_id = self.vocab.get(char, self.vocab['<UNK>'])\n",
        "                ids.append(token_id)\n",
        "        else:\n",
        "            # Word-level encoding\n",
        "            words = self._tokenize_text(text)\n",
        "            ids = []\n",
        "            for word in words:\n",
        "                token_id = self.vocab.get(word, self.vocab['<UNK>'])\n",
        "                ids.append(token_id)\n",
        "\n",
        "        # Add sequence markers\n",
        "        return [self.vocab['<SOS>']] + ids + [self.vocab['<EOS>']]\n",
        "\n",
        "    def ids_to_text(self, ids: List[int]) -> str:\n",
        "        \"\"\"Convert token IDs back to readable text\"\"\"\n",
        "        tokens = []\n",
        "        for id_val in ids:\n",
        "            token = self.id_to_token.get(id_val, '<UNK>')\n",
        "            # Skip special tokens for display\n",
        "            if token not in ['<SOS>', '<EOS>', '<PAD>']:\n",
        "                tokens.append(token)\n",
        "\n",
        "        if self.use_char_level:\n",
        "            return ''.join(tokens)\n",
        "        else:\n",
        "            return ' '.join(tokens)\n",
        "\n",
        "    def _print_vocab_stats(self):\n",
        "        \"\"\"Print vocabulary statistics\"\"\"\n",
        "        print(f\"\\nüìà Vocabulary Statistics:\")\n",
        "        print(f\"   Final vocabulary size: {self.vocab_size}\")\n",
        "        print(f\"   Special tokens: {len(self.special_tokens)}\")\n",
        "        print(f\"   Content tokens: {self.vocab_size - len(self.special_tokens)}\")\n",
        "\n",
        "        if self.vocab_stats:\n",
        "            for key, value in self.vocab_stats.items():\n",
        "                if isinstance(value, float):\n",
        "                    print(f\"   {key.replace('_', ' ').title()}: {value:.3f}\")\n",
        "                else:\n",
        "                    print(f\"   {key.replace('_', ' ').title()}: {value:,}\")\n",
        "\n",
        "    def get_vocab_sample(self, n=20):\n",
        "        \"\"\"Get a sample of vocabulary items for inspection\"\"\"\n",
        "        vocab_items = list(self.vocab.items())\n",
        "        special_count = len(self.special_tokens)\n",
        "\n",
        "        print(f\"\\nüîç Vocabulary Sample:\")\n",
        "        print(\"Special tokens:\")\n",
        "        for token, id_val in vocab_items[:special_count]:\n",
        "            print(f\"   {id_val:3d}: '{token}'\")\n",
        "\n",
        "        print(\"Content tokens (sample):\")\n",
        "        sample_items = vocab_items[special_count:special_count + n]\n",
        "        for token, id_val in sample_items:\n",
        "            display_token = repr(token) if self.use_char_level else token\n",
        "            print(f\"   {id_val:3d}: {display_token}\")\n",
        "\n",
        "        if len(vocab_items) > special_count + n:\n",
        "            print(f\"   ... and {len(vocab_items) - special_count - n} more\")\n",
        "\n",
        "    def save_vocabulary(self, filepath: str):\n",
        "        \"\"\"Save vocabulary and configuration to file\"\"\"\n",
        "        vocab_data = {\n",
        "            'vocab': self.vocab,\n",
        "            'vocab_size': self.vocab_size,\n",
        "            'id_to_token': self.id_to_token,\n",
        "            'use_char_level': self.use_char_level,\n",
        "            'special_tokens': self.special_tokens,\n",
        "            'vocab_stats': self.vocab_stats,\n",
        "            'config_snapshot': {\n",
        "                'sample_rate': config.SAMPLE_RATE,\n",
        "                'max_seq_len': config.MAX_SEQ_LEN,\n",
        "                'mel_dim': config.MEL_DIM\n",
        "            }\n",
        "        }\n",
        "\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(vocab_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"üíæ Vocabulary saved to {filepath}\")\n",
        "        print(f\"   Size: {os.path.getsize(filepath) / 1024:.1f} KB\")\n",
        "\n",
        "    def load_vocabulary(self, filepath: str):\n",
        "        \"\"\"Load vocabulary from file\"\"\"\n",
        "        try:\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                vocab_data = json.load(f)\n",
        "\n",
        "            self.vocab = vocab_data['vocab']\n",
        "            self.vocab_size = vocab_data['vocab_size']\n",
        "            self.id_to_token = {int(k): v for k, v in vocab_data['id_to_token'].items()}\n",
        "            self.use_char_level = vocab_data.get('use_char_level', True)\n",
        "            self.special_tokens = vocab_data.get('special_tokens', self.special_tokens)\n",
        "            self.vocab_stats = vocab_data.get('vocab_stats', {})\n",
        "\n",
        "            # Update config\n",
        "            config.VOCAB_SIZE = self.vocab_size\n",
        "\n",
        "            print(f\"üìñ Vocabulary loaded from {filepath}\")\n",
        "            print(f\"   Size: {self.vocab_size}\")\n",
        "            print(f\"   Type: {'Character-level' if self.use_char_level else 'Word-level'}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to load vocabulary: {e}\")\n",
        "            return False\n",
        "\n",
        "# Build text processor if LibriSpeech is available\n",
        "text_processor = None\n",
        "\n",
        "if librispeech_samples:\n",
        "    print(\"\\nüèóÔ∏è Building text processor for LibriSpeech...\")\n",
        "\n",
        "    # Choose tokenization strategy\n",
        "    use_char_level = True  # Set to False for word-level tokenization\n",
        "\n",
        "    text_processor = LibriSpeechTextProcessor(use_char_level=use_char_level)\n",
        "    text_processor.build_vocabulary(\n",
        "        librispeech_samples,\n",
        "        min_freq=5 if not use_char_level else 1,\n",
        "        max_vocab_size=5000\n",
        "    )\n",
        "\n",
        "    # Test the text processor\n",
        "    if librispeech_samples:\n",
        "        test_text = librispeech_samples[0]['transcript'][:100]\n",
        "        test_ids = text_processor.text_to_ids(test_text)\n",
        "        reconstructed = text_processor.ids_to_text(test_ids)\n",
        "\n",
        "        print(f\"\\nüß™ Text Processor Test:\")\n",
        "        print(f\"   Original: '{test_text}'\")\n",
        "        print(f\"   Token IDs: {test_ids[:15]}... (showing first 15)\")\n",
        "        print(f\"   Reconstructed: '{reconstructed}'\")\n",
        "        print(f\"   Tokens match: {test_text.upper() == reconstructed.upper()}\")\n",
        "\n",
        "    # Show vocabulary sample\n",
        "    text_processor.get_vocab_sample(n=15)\n",
        "\n",
        "    # Save vocabulary\n",
        "    text_processor.save_vocabulary(\"librispeech_vocab.json\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå Cannot build text processor - no LibriSpeech samples available\")\n",
        "\n",
        "# ==============================================================================\n",
        "# CELL 5: Utility Functions for Audio and Visualization\n",
        "# ==============================================================================\n",
        "\n",
        "def plot_mel_spectrogram(mel_spec, title=\"Mel-Spectrogram\", figsize=(12, 4), save_path=None):\n",
        "    \"\"\"Plot mel-spectrogram with proper formatting\"\"\"\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    # Handle different input formats\n",
        "    if len(mel_spec.shape) == 2:\n",
        "        # [time, mel_dim] format\n",
        "        plt.imshow(mel_spec.T, aspect='auto', origin='lower', cmap='viridis')\n",
        "    else:\n",
        "        # [mel_dim, time] format\n",
        "        plt.imshow(mel_spec, aspect='auto', origin='lower', cmap='viridis')\n",
        "\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title(title)\n",
        "    plt.ylabel('Mel Frequency Bin')\n",
        "    plt.xlabel('Time Frame')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"üìä Plot saved to {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def play_audio(audio, sample_rate=16000, title=\"Audio\", autoplay=False):\n",
        "    \"\"\"Play audio in notebook with enhanced interface\"\"\"\n",
        "    print(f\"üîä {title}\")\n",
        "    print(f\"   Duration: {len(audio)/sample_rate:.2f}s, Samples: {len(audio):,}\")\n",
        "\n",
        "    # Normalize audio to prevent clipping\n",
        "    if np.max(np.abs(audio)) > 0:\n",
        "        audio_normalized = audio / np.max(np.abs(audio)) * 0.9\n",
        "    else:\n",
        "        audio_normalized = audio\n",
        "\n",
        "    display(Audio(audio_normalized, rate=sample_rate, autoplay=autoplay))\n",
        "\n",
        "def plot_audio_waveform(audio, sample_rate=16000, title=\"Audio Waveform\", figsize=(15, 4)):\n",
        "    \"\"\"Plot audio waveform with time axis\"\"\"\n",
        "    plt.figure(figsize=figsize)\n",
        "    time_axis = np.linspace(0, len(audio)/sample_rate, len(audio))\n",
        "    plt.plot(time_axis, audio)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Time (seconds)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def show_training_progress(losses, val_losses=None, title=\"Training Progress\", figsize=(12, 5)):\n",
        "    \"\"\"Enhanced training progress visualization\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2 if val_losses else 1, figsize=figsize)\n",
        "\n",
        "    if val_losses:\n",
        "        # Loss plot\n",
        "        axes[0].plot(losses, label='Training Loss', linewidth=2)\n",
        "        axes[0].plot(val_losses, label='Validation Loss', linewidth=2)\n",
        "        axes[0].set_title('Loss Progress')\n",
        "        axes[0].set_xlabel('Epoch')\n",
        "        axes[0].set_ylabel('Loss')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Learning curve\n",
        "        if len(losses) > 5:\n",
        "            recent_train = np.mean(losses[-5:])\n",
        "            recent_val = np.mean(val_losses[-5:]) if val_losses else 0\n",
        "            overfitting_indicator = recent_val / recent_train if recent_train > 0 else 1\n",
        "\n",
        "            axes[1].text(0.1, 0.8, f\"Recent Train Loss: {recent_train:.4f}\", transform=axes[1].transAxes)\n",
        "            axes[1].text(0.1, 0.7, f\"Recent Val Loss: {recent_val:.4f}\", transform=axes[1].transAxes)\n",
        "            axes[1].text(0.1, 0.6, f\"Overfitting Ratio: {overfitting_indicator:.3f}\", transform=axes[1].transAxes)\n",
        "\n",
        "            if overfitting_indicator > 1.2:\n",
        "                axes[1].text(0.1, 0.5, \"‚ö†Ô∏è Possible overfitting\", transform=axes[1].transAxes, color='red')\n",
        "            else:\n",
        "                axes[1].text(0.1, 0.5, \"‚úÖ Training looks good\", transform=axes[1].transAxes, color='green')\n",
        "\n",
        "        axes[1].set_title('Training Status')\n",
        "        axes[1].axis('off')\n",
        "    else:\n",
        "        # Single loss plot\n",
        "        if isinstance(axes, np.ndarray):\n",
        "            ax = axes[0]\n",
        "        else:\n",
        "            ax = axes\n",
        "        ax.plot(losses, label='Training Loss', linewidth=2)\n",
        "        ax.set_title(title)\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel('Loss')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def demo_dataset_samples(dataset, num_samples=3):\n",
        "    \"\"\"Comprehensive demonstration of LibriSpeech dataset samples\"\"\"\n",
        "    if dataset is None:\n",
        "        print(\"‚ùå No dataset available for demo\")\n",
        "        return\n",
        "\n",
        "    print(f\"üéµ LibriSpeech Dataset Sample Demo\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Showing {min(num_samples, len(dataset))} samples from dataset\")\n",
        "\n",
        "    for i in range(min(num_samples, len(dataset))):\n",
        "        print(f\"\\nüìù Sample {i+1}:\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        try:\n",
        "            # Get processed dataset item\n",
        "            text_tensor, mel_tensor, duration_tensor, text_len = dataset[i]\n",
        "\n",
        "            # Get original sample info\n",
        "            info = dataset.get_sample_info(i)\n",
        "\n",
        "            print(f\"File ID: {info['file_id']}\")\n",
        "            print(f\"Speaker: {info['speaker_id']}\")\n",
        "            print(f\"Chapter: {info['chapter_id']}\")\n",
        "            print(f\"Duration: {info['duration']:.2f}s\")\n",
        "            print(f\"Text: '{info['transcript']}'\")\n",
        "            print(f\"Text length: {len(info['transcript'])} chars\")\n",
        "            print(f\"Tokenized length: {text_len} tokens\")\n",
        "            print(f\"Mel shape: {mel_tensor.shape}\")\n",
        "            print(f\"Duration tensor shape: {duration_tensor.shape}\")\n",
        "\n",
        "            # Load and play original audio\n",
        "            try:\n",
        "                waveform, sr = torchaudio.load(info['audio_path'])\n",
        "                waveform = waveform.squeeze(0).numpy()\n",
        "\n",
        "                # Resample if necessary\n",
        "                if sr != config.SAMPLE_RATE:\n",
        "                    waveform = librosa.resample(\n",
        "                        waveform,\n",
        "                        orig_sr=sr,\n",
        "                        target_sr=config.SAMPLE_RATE\n",
        "                    )\n",
        "\n",
        "                print(f\"Audio samples: {len(waveform):,}\")\n",
        "                play_audio(waveform, config.SAMPLE_RATE, f\"Sample {i+1} - Original LibriSpeech Audio\")\n",
        "\n",
        "                # Show mel-spectrogram for first sample only\n",
        "                if i == 0:\n",
        "                    print(f\"\\nüìä Mel-spectrogram visualization:\")\n",
        "                    plot_mel_spectrogram(\n",
        "                        mel_tensor.numpy(),\n",
        "                        f\"Sample {i+1} - Mel-Spectrogram ({mel_tensor.shape[0]} frames)\"\n",
        "                    )\n",
        "\n",
        "                # Show waveform for first sample\n",
        "                if i == 0:\n",
        "                    plot_audio_waveform(\n",
        "                        waveform[:config.SAMPLE_RATE*3],  # First 3 seconds\n",
        "                        config.SAMPLE_RATE,\n",
        "                        f\"Sample {i+1} - Audio Waveform (first 3s)\"\n",
        "                    )\n",
        "\n",
        "            except Exception as audio_error:\n",
        "                print(f\"‚ùå Could not load/process audio: {audio_error}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing sample {i+1}: {e}\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Dataset demo completed\")\n",
        "\n",
        "def test_text_processor(text_processor, test_texts=None):\n",
        "    \"\"\"Comprehensive test of text processor functionality\"\"\"\n",
        "    if text_processor is None:\n",
        "        print(\"‚ùå No text processor available for testing\")\n",
        "        return\n",
        "\n",
        "    print(f\"üß™ Text Processor Testing\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Default test texts if none provided\n",
        "    if test_texts is None:\n",
        "        test_texts = [\n",
        "            \"HELLO WORLD\",\n",
        "            \"THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG\",\n",
        "            \"TESTING ONE TWO THREE\",\n",
        "            \"MACHINE LEARNING IS AMAZING\"\n",
        "        ]\n",
        "\n",
        "    for i, text in enumerate(test_texts, 1):\n",
        "        print(f\"\\nTest {i}: '{text}'\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "        try:\n",
        "            # Convert to IDs and back\n",
        "            token_ids = text_processor.text_to_ids(text)\n",
        "            reconstructed = text_processor.ids_to_text(token_ids)\n",
        "\n",
        "            print(f\"Original: '{text}'\")\n",
        "            print(f\"Token IDs: {token_ids}\")\n",
        "            print(f\"ID count: {len(token_ids)}\")\n",
        "            print(f\"Reconstructed: '{reconstructed}'\")\n",
        "\n",
        "            # Check reconstruction quality\n",
        "            if text.upper() == reconstructed.upper():\n",
        "                print(\"‚úÖ Perfect reconstruction\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è Reconstruction differs from original\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing text: {e}\")\n",
        "\n",
        "    # Vocabulary statistics\n",
        "    print(f\"\\nüìä Vocabulary Info:\")\n",
        "    print(f\"Size: {text_processor.vocab_size}\")\n",
        "    print(f\"Type: {'Character-level' if text_processor.use_char_level else 'Word-level'}\")\n",
        "\n",
        "    # Show some vocabulary samples\n",
        "    text_processor.get_vocab_sample(n=10)\n",
        "\n",
        "def validate_system_setup():\n",
        "    \"\"\"Comprehensive system validation\"\"\"\n",
        "    print(f\"üîç System Setup Validation\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    checks = []\n",
        "\n",
        "    # LibriSpeech data check\n",
        "    if librispeech_samples and len(librispeech_samples) > 0:\n",
        "        checks.append((\"LibriSpeech Data\", True, f\"{len(librispeech_samples)} samples loaded\"))\n",
        "    else:\n",
        "        checks.append((\"LibriSpeech Data\", False, \"No samples loaded - check path\"))\n",
        "\n",
        "    # Text processor check\n",
        "    if text_processor and text_processor.vocab_size > len(text_processor.special_tokens):\n",
        "        checks.append((\"Text Processor\", True, f\"Vocabulary size: {text_processor.vocab_size}\"))\n",
        "    else:\n",
        "        checks.append((\"Text Processor\", False, \"Not properly initialized\"))\n",
        "\n",
        "    # CUDA availability\n",
        "    if torch.cuda.is_available():\n",
        "        memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        checks.append((\"CUDA Support\", True, f\"GPU available: {memory_gb:.1f}GB\"))\n",
        "    else:\n",
        "        checks.append((\"CUDA Support\", False, \"GPU not available - will use CPU\"))\n",
        "\n",
        "    # Audio libraries\n",
        "    try:\n",
        "        import librosa\n",
        "        import soundfile\n",
        "        checks.append((\"Audio Libraries\", True, \"librosa and soundfile available\"))\n",
        "    except ImportError as e:\n",
        "        checks.append((\"Audio Libraries\", False, f\"Missing libraries: {e}\"))\n",
        "\n",
        "    # Display results\n",
        "    print(\"System Component Status:\")\n",
        "    for component, status, details in checks:\n",
        "        status_icon = \"‚úÖ\" if status else \"‚ùå\"\n",
        "        print(f\"   {status_icon} {component}: {details}\")\n",
        "\n",
        "    # Overall status\n",
        "    all_critical_ok = all(check[1] for check in checks[:2])  # LibriSpeech and text processor are critical\n",
        "    if all_critical_ok:\n",
        "        print(f\"\\nüéâ System ready for TTS training and inference!\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è System has issues that need to be resolved before proceeding\")\n",
        "\n",
        "    return all_critical_ok\n",
        "\n",
        "def get_system_info():\n",
        "    \"\"\"Get comprehensive system information\"\"\"\n",
        "    info = {\n",
        "        'pytorch_version': torch.__version__,\n",
        "        'cuda_available': torch.cuda.is_available(),\n",
        "        'device': str(config.DEVICE),\n",
        "        'librispeech_samples': len(librispeech_samples) if librispeech_samples else 0,\n",
        "        'vocab_size': text_processor.vocab_size if text_processor else 0,\n",
        "        'tokenization_type': 'char' if text_processor and text_processor.use_char_level else 'word',\n",
        "        'sample_rate': config.SAMPLE_RATE,\n",
        "        'batch_size': config.BATCH_SIZE,\n",
        "        'hidden_dim': config.HIDDEN_DIM\n",
        "    }\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        info['gpu_name'] = torch.cuda.get_device_name()\n",
        "        info['gpu_memory'] = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "\n",
        "    return info\n",
        "\n",
        "# ==============================================================================\n",
        "# CELL 6: Audio Processing and Dataset Implementation\n",
        "# ==============================================================================\n",
        "\n",
        "class LibriSpeechTTSDataset(Dataset):\n",
        "    \"\"\"Production-ready LibriSpeech dataset for TTS training with comprehensive preprocessing\"\"\"\n",
        "\n",
        "    def __init__(self, samples: List[dict], text_processor,\n",
        "                 max_audio_length: float = 15.0, min_audio_length: float = 1.0):\n",
        "        self.samples = samples\n",
        "        self.text_processor = text_processor\n",
        "        self.max_audio_length = max_audio_length\n",
        "        self.min_audio_length = min_audio_length\n",
        "\n",
        "        # Audio processing parameters\n",
        "        self.sample_rate = config.SAMPLE_RATE\n",
        "        self.hop_length = config.HOP_LENGTH\n",
        "        self.n_mels = config.MEL_DIM\n",
        "        self.n_fft = config.N_FFT\n",
        "        self.win_length = config.WIN_LENGTH\n",
        "        self.f_min = config.F_MIN\n",
        "        self.f_max = config.F_MAX\n",
        "        self.max_seq_len = config.MAX_SEQ_LEN\n",
        "\n",
        "        # Filter and process samples\n",
        "        self.filtered_samples = self._filter_samples()\n",
        "\n",
        "        # Statistics\n",
        "        self.stats = self._compute_dataset_stats()\n",
        "\n",
        "        print(f\"üìä Dataset initialized:\")\n",
        "        print(f\"   Total samples: {len(self.filtered_samples)}\")\n",
        "        print(f\"   Filtered from: {len(samples)} original samples\")\n",
        "        print(f\"   Filter rate: {len(self.filtered_samples)/len(samples)*100:.1f}%\")\n",
        "        self._print_stats()\n",
        "\n",
        "    def _filter_samples(self):\n",
        "        \"\"\"Filter samples by audio duration and text length with detailed logging\"\"\"\n",
        "        print(\"üîç Filtering samples by duration and text length...\")\n",
        "\n",
        "        filtered = []\n",
        "        failed_loads = 0\n",
        "        duration_filtered = 0\n",
        "        text_filtered = 0\n",
        "\n",
        "        for sample in tqdm(self.samples, desc=\"Filtering samples\"):\n",
        "            try:\n",
        "                # Quick duration check without loading full audio\n",
        "                audio_info = torchaudio.info(sample['audio_path'])\n",
        "                duration = audio_info.num_frames / audio_info.sample_rate\n",
        "\n",
        "                # Duration filter\n",
        "                if not (self.min_audio_length <= duration <= self.max_audio_length):\n",
        "                    duration_filtered += 1\n",
        "                    continue\n",
        "\n",
        "                # Text length filter\n",
        "                text_len = len(sample['transcript'])\n",
        "                if text_len > self.max_seq_len * 2:  # Rough estimate for max tokens\n",
        "                    text_filtered += 1\n",
        "                    continue\n",
        "\n",
        "                # Add duration info and keep sample\n",
        "                sample['duration'] = duration\n",
        "                filtered.append(sample)\n",
        "\n",
        "            except Exception as e:\n",
        "                failed_loads += 1\n",
        "                if failed_loads <= 5:  # Show first few errors\n",
        "                    print(f\"‚ö†Ô∏è Failed to load {sample['audio_path']}: {e}\")\n",
        "\n",
        "        # Print filtering summary\n",
        "        print(f\"üìã Filtering Summary:\")\n",
        "        print(f\"   Duration filtered: {duration_filtered}\")\n",
        "        print(f\"   Text length filtered: {text_filtered}\")\n",
        "        print(f\"   Failed loads: {failed_loads}\")\n",
        "        print(f\"   Kept: {len(filtered)}\")\n",
        "\n",
        "        return filtered\n",
        "\n",
        "    def _compute_dataset_stats(self):\n",
        "        \"\"\"Compute comprehensive dataset statistics\"\"\"\n",
        "        if not self.filtered_samples:\n",
        "            return {}\n",
        "\n",
        "        durations = [s['duration'] for s in self.filtered_samples]\n",
        "        text_lengths = [len(s['transcript']) for s in self.filtered_samples]\n",
        "        speakers = set(s['speaker_id'] for s in self.filtered_samples)\n",
        "\n",
        "        stats = {\n",
        "            'num_samples': len(self.filtered_samples),\n",
        "            'num_speakers': len(speakers),\n",
        "            'duration_mean': np.mean(durations),\n",
        "            'duration_std': np.std(durations),\n",
        "            'duration_min': np.min(durations),\n",
        "            'duration_max': np.max(durations),\n",
        "            'text_length_mean': np.mean(text_lengths),\n",
        "            'text_length_std': np.std(text_lengths),\n",
        "            'total_duration_hours': sum(durations) / 3600\n",
        "        }\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def _print_stats(self):\n",
        "        \"\"\"Print dataset statistics\"\"\"\n",
        "        if not self.stats:\n",
        "            return\n",
        "\n",
        "        print(f\"\\nüìà Dataset Statistics:\")\n",
        "        print(f\"   Samples: {self.stats['num_samples']:,}\")\n",
        "        print(f\"   Speakers: {self.stats['num_speakers']:,}\")\n",
        "        print(f\"   Total duration: {self.stats['total_duration_hours']:.1f} hours\")\n",
        "        print(f\"   Avg duration: {self.stats['duration_mean']:.1f}s ¬± {self.stats['duration_std']:.1f}s\")\n",
        "        print(f\"   Duration range: {self.stats['duration_min']:.1f}s - {self.stats['duration_max']:.1f}s\")\n",
        "        print(f\"   Avg text length: {self.stats['text_length_mean']:.0f} ¬± {self.stats['text_length_std']:.0f} chars\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filtered_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Get a processed sample with comprehensive error handling\"\"\"\n",
        "        sample = self.filtered_samples[idx]\n",
        "\n",
        "        try:\n",
        "            # Load and preprocess audio\n",
        "            waveform, sample_rate = torchaudio.load(sample['audio_path'])\n",
        "            waveform = waveform.squeeze(0).numpy()\n",
        "\n",
        "            # Resample if necessary\n",
        "            if sample_rate != self.sample_rate:\n",
        "                waveform = librosa.resample(\n",
        "                    waveform,\n",
        "                    orig_sr=sample_rate,\n",
        "                    target_sr=self.sample_rate\n",
        "                )\n",
        "\n",
        "            # Convert to mel-spectrogram\n",
        "            mel_spec = self._compute_mel_spectrogram(waveform)\n",
        "\n",
        "            # Process text\n",
        "            text = sample['transcript']\n",
        "            text_ids = self.text_processor.text_to_ids(text)\n",
        "\n",
        "            # Truncate if too long\n",
        "            if len(text_ids) > self.max_seq_len:\n",
        "                text_ids = text_ids[:self.max_seq_len-1] + [self.text_processor.vocab['<EOS>']]\n",
        "\n",
        "            # Compute target durations (distribute mel frames across text tokens)\n",
        "            mel_length = mel_spec.shape[0]\n",
        "            text_length = len(text_ids)\n",
        "\n",
        "            if text_length > 0:\n",
        "                # Base duration per token\n",
        "                base_duration = mel_length / text_length\n",
        "                durations = torch.ones(text_length, dtype=torch.float) * base_duration\n",
        "\n",
        "                # Add natural variation for more realistic durations\n",
        "                if text_length > 2:\n",
        "                    # Special tokens get shorter durations\n",
        "                    durations[0] *= 0.5   # SOS token\n",
        "                    durations[-1] *= 0.5  # EOS token\n",
        "\n",
        "                    # Add realistic duration variation\n",
        "                    variation = torch.randn(text_length) * 0.3 + 1.0\n",
        "                    variation = torch.clamp(variation, 0.3, 3.0)\n",
        "                    durations *= variation\n",
        "\n",
        "                # Normalize to match total mel length\n",
        "                durations = durations * (mel_length / durations.sum())\n",
        "            else:\n",
        "                durations = torch.tensor([mel_length], dtype=torch.float)\n",
        "\n",
        "            # Convert to tensors\n",
        "            text_tensor = torch.tensor(text_ids, dtype=torch.long)\n",
        "            mel_tensor = torch.tensor(mel_spec, dtype=torch.float)  # [time, mel_dim]\n",
        "\n",
        "            return text_tensor, mel_tensor, durations, len(text_ids)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Return dummy data if sample processing fails\n",
        "            print(f\"‚ùå Error processing sample {idx} ({sample.get('file_id', 'unknown')}): {e}\")\n",
        "\n",
        "            # Create dummy tensors\n",
        "            dummy_text = torch.tensor([\n",
        "                self.text_processor.vocab['< SOS >'],\n",
        "                self.text_processor.vocab['<UNK>'],\n",
        "                self.text_processor.vocab['<EOS>']\n",
        "            ], dtype=torch.long)\n",
        "            dummy_mel = torch.zeros(100, self.n_mels, dtype=torch.float)\n",
        "            dummy_duration = torch.ones(3, dtype=torch.float) * 33.33  # 100/3\n",
        "\n",
        "            return dummy_text, dummy_mel, dummy_duration, 3\n",
        "\n",
        "    def _compute_mel_spectrogram(self, waveform: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Compute mel-spectrogram with LibriSpeech-optimized parameters\"\"\"\n",
        "        # Compute mel-spectrogram using librosa\n",
        "        mel_spec = librosa.feature.melspectrogram(\n",
        "            y=waveform,\n",
        "            sr=self.sample_rate,\n",
        "            hop_length=self.hop_length,\n",
        "            n_fft=self.n_fft,\n",
        "            win_length=self.win_length,\n",
        "            n_mels=self.n_mels,\n",
        "            fmin=self.f_min,\n",
        "            fmax=self.f_max\n",
        "        )\n",
        "\n",
        "        # Convert to log scale (dB)\n",
        "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "        # Normalize to [-1, 1] range for training stability\n",
        "        mel_min, mel_max = mel_spec.min(), mel_spec.max()\n",
        "        if mel_max > mel_min:\n",
        "            mel_spec = (mel_spec - mel_min) / (mel_max - mel_min) * 2 - 1\n",
        "        else:\n",
        "            mel_spec = np.zeros_like(mel_spec)\n",
        "\n",
        "        return mel_spec.T  # Return as [time, mel_dim] for consistency\n",
        "\n",
        "    def get_sample_info(self, idx):\n",
        "        \"\"\"Get detailed information about a sample\"\"\"\n",
        "        if idx >= len(self.filtered_samples):\n",
        "            return None\n",
        "\n",
        "        sample = self.filtered_samples[idx]\n",
        "        return {\n",
        "            'file_id': sample['file_id'],\n",
        "            'transcript': sample['transcript'],\n",
        "            'audio_path': sample['audio_path'],\n",
        "            'duration': sample.get('duration', 'unknown'),\n",
        "            'speaker_id': sample['speaker_id'],\n",
        "            'chapter_id': sample['chapter_id']\n",
        "        }\n",
        "\n",
        "    def get_random_samples(self, n=5):\n",
        "        \"\"\"Get information about n random samples\"\"\"\n",
        "        if len(self.filtered_samples) == 0:\n",
        "            return []\n",
        "\n",
        "        indices = np.random.choice(len(self.filtered_samples), min(n, len(self.filtered_samples)), replace=False)\n",
        "        return [self.get_sample_info(i) for i in indices]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Advanced collate function for variable length sequences with proper padding\"\"\"\n",
        "    text_tensors, mel_tensors, duration_tensors, text_lengths = zip(*batch)\n",
        "\n",
        "    # Pad text sequences to same length\n",
        "    max_text_len = max(len(t) for t in text_tensors)\n",
        "    padded_texts = []\n",
        "    padded_durations = []\n",
        "\n",
        "    pad_token = text_processor.vocab['<PAD>'] if text_processor else 0\n",
        "\n",
        "    for text, duration in zip(text_tensors, duration_tensors):\n",
        "        pad_len = max_text_len - len(text)\n",
        "        padded_text = F.pad(text, (0, pad_len), value=pad_token)\n",
        "        padded_duration = F.pad(duration, (0, pad_len), value=0)\n",
        "        padded_texts.append(padded_text)\n",
        "        padded_durations.append(padded_duration)\n",
        "\n",
        "    # Pad mel sequences to same length\n",
        "    max_mel_len = max(mel.size(0) for mel in mel_tensors)\n",
        "    padded_mels = []\n",
        "\n",
        "    for mel in mel_tensors:\n",
        "        pad_len = max_mel_len - mel.size(0)\n",
        "        padded_mel = F.pad(mel, (0, 0, 0, pad_len), value=0)  # Pad time dimension\n",
        "        padded_mels.append(padded_mel)\n",
        "\n",
        "    return (\n",
        "        torch.stack(padded_texts),      # [batch, max_text_len]\n",
        "        torch.stack(padded_mels),       # [batch, max_mel_len, mel_dim]\n",
        "        torch.stack(padded_durations),  # [batch, max_text_len]\n",
        "        torch.tensor(text_lengths)      # [batch]\n",
        "    )\n",
        "\n",
        "def create_data_loaders(samples, text_processor, train_ratio=0.95, batch_size=None, num_workers=2):\n",
        "    \"\"\"Create train and validation data loaders with comprehensive setup\"\"\"\n",
        "    if batch_size is None:\n",
        "        batch_size = config.BATCH_SIZE\n",
        "\n",
        "    print(f\"üì¶ Creating data loaders...\")\n",
        "    print(f\"   Batch size: {batch_size}\")\n",
        "    print(f\"   Train ratio: {train_ratio}\")\n",
        "    print(f\"   Num workers: {num_workers}\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = LibriSpeechTTSDataset(\n",
        "        samples,\n",
        "        text_processor,\n",
        "        max_audio_length=config.MAX_AUDIO_LENGTH,\n",
        "        min_audio_length=config.MIN_AUDIO_LENGTH\n",
        "    )\n",
        "\n",
        "    if len(dataset) == 0:\n",
        "        print(\"‚ùå Dataset is empty - cannot create data loaders\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Split dataset into train and validation\n",
        "    total_size = len(dataset)\n",
        "    train_size = int(train_ratio * total_size)\n",
        "    val_size = total_size - train_size\n",
        "\n",
        "    print(f\"üìä Dataset split:\")\n",
        "    print(f\"   Total: {total_size}\")\n",
        "    print(f\"   Training: {train_size}\")\n",
        "    print(f\"   Validation: {val_size}\")\n",
        "\n",
        "    # Use fixed seed for reproducible splits\n",
        "    generator = torch.Generator().manual_seed(42)\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size], generator=generator\n",
        "    )\n",
        "\n",
        "    # Create data loaders with optimized settings\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "        drop_last=True,  # Ensure consistent batch sizes\n",
        "        persistent_workers=num_workers > 0\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "        drop_last=True,\n",
        "        persistent_workers=num_workers > 0\n",
        "    )\n",
        "\n",
        "    print(f\"‚úÖ Data loaders created:\")\n",
        "    print(f\"   Training batches: {len(train_loader)}\")\n",
        "    print(f\"   Validation batches: {len(val_loader)}\")\n",
        "    print(f\"   Samples per epoch: {len(train_loader) * batch_size}\")\n",
        "\n",
        "    return train_loader, val_loader, dataset\n",
        "\n",
        "# Create dataset and data loaders if everything is available\n",
        "train_dataloader = None\n",
        "val_dataloader = None\n",
        "dataset = None\n",
        "\n",
        "if librispeech_samples and text_processor:\n",
        "    print(\"\\nüöÄ Creating LibriSpeech TTS dataset and data loaders...\")\n",
        "\n",
        "    try:\n",
        "        train_dataloader, val_dataloader, dataset = create_data_loaders(\n",
        "            librispeech_samples,\n",
        "            text_processor,\n",
        "            train_ratio=config.TRAIN_RATIO,\n",
        "            batch_size=config.BATCH_SIZE,\n",
        "            num_workers=2\n",
        "        )\n",
        "\n",
        "        if dataset and len(dataset) > 0:\n",
        "            print(f\"\\nüìã Dataset ready for training!\")\n",
        "\n",
        "            # Show sample entries\n",
        "            print(f\"\\nSample dataset entries:\")\n",
        "            random_samples = dataset.get_random_samples(3)\n",
        "            for i, info in enumerate(random_samples, 1):\n",
        "                transcript_preview = info['transcript'][:80] + \"...\" if len(info['transcript']) > 80 else info['transcript']\n",
        "                print(f\"   {i}. {info['file_id']}: '{transcript_preview}'\")\n",
        "                print(f\"      Speaker: {info['speaker_id']}, Duration: {info['duration']:.2f}s\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to create dataset: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå Cannot create dataset - missing LibriSpeech samples or text processor\")\n",
        "    print(\"   Please ensure LibriSpeech is loaded and text processor is initialized\")\n",
        "\n",
        "# ==============================================================================\n",
        "# PART 1 SUMMARY AND VALIDATION\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ PART 1 COMPLETION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Validate all components\n",
        "system_ready = validate_system_setup()\n",
        "\n",
        "# Print system information\n",
        "system_info = get_system_info()\n",
        "print(f\"\\nüìä System Configuration:\")\n",
        "for key, value in system_info.items():\n",
        "    print(f\"   {key.replace('_', ' ').title()}: {value}\")\n",
        "\n",
        "# Demo dataset if available\n",
        "if dataset and len(dataset) > 0:\n",
        "    print(f\"\\nüéµ Running dataset demo...\")\n",
        "    demo_dataset_samples(dataset, num_samples=2)\n",
        "\n",
        "# Test text processor if available\n",
        "if text_processor:\n",
        "    print(f\"\\nüß™ Testing text processor...\")\n",
        "    test_text_processor(text_processor)\n",
        "\n",
        "# Next steps\n",
        "print(f\"\\nüöÄ PART 1 COMPLETE - Ready for Part 2!\")\n",
        "print(f\"Part 1 provided:\")\n",
        "print(f\"   ‚úÖ Complete LibriSpeech data loading ({len(librispeech_samples) if librispeech_samples else 0} samples)\")\n",
        "print(f\"   ‚úÖ Advanced text processing (vocab size: {text_processor.vocab_size if text_processor else 0})\")\n",
        "print(f\"   ‚úÖ Dataset implementation with preprocessing\")\n",
        "print(f\"   ‚úÖ Data loaders for training\")\n",
        "print(f\"   ‚úÖ Comprehensive utilities and validation\")\n",
        "print(f\"\\nNext: Part 2 will implement the neural network architecture\")\n",
        "print(f\"      (Streaming components, attention mechanisms, duration prediction)\")\n",
        "\n",
        "if not system_ready:\n",
        "    print(f\"\\n‚ö†Ô∏è IMPORTANT: Please resolve the system issues above before proceeding to Part 2\")\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "5B7RoHZIYwo2",
        "outputId": "acde8630-24df-4ce9-c574-4a8d83e4b09e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üéâ REAL-TIME STREAMING TTS SYSTEM - PART 1\n",
            "   Foundation, Data Loading, and Text Processing\n",
            "============================================================\n",
            "‚úÖ All imports successful\n",
            "PyTorch version: 2.6.0+cpu\n",
            "CUDA available: False\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, maia, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: tpu",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1216767331>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# ==============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTTSConfig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;34m\"\"\"Complete configuration for LibriSpeech TTS system\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-1216767331>\u001b[0m in \u001b[0;36mTTSConfig\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# === DEVICE CONFIGURATION ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mDEVICE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'tpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0mUSE_MIXED_PRECISION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, maia, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: tpu"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Real-Time Streaming Text-to-Speech System - PART 2\n",
        "# Neural Architecture: Streaming Components, Attention, and Model Implementation\n",
        "# Built from scratch for LibriSpeech train-clean-100\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üß† REAL-TIME STREAMING TTS SYSTEM - PART 2\")\n",
        "print(\"   Neural Architecture: Streaming Components & Attention Mechanisms\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ==============================================================================\n",
        "# CELL 7: Streaming Buffer and Core Components\n",
        "# ==============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "import queue\n",
        "from typing import List, Optional, Tuple\n",
        "import time\n",
        "\n",
        "class StreamingBuffer:\n",
        "    \"\"\"Advanced streaming buffer for real-time text processing with overlap management\"\"\"\n",
        "\n",
        "    def __init__(self, buffer_size: int = 4, overlap_frames: int = 2):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.overlap_frames = overlap_frames\n",
        "        self.text_buffer = []\n",
        "        self.audio_buffer = queue.Queue()\n",
        "        self.processed_count = 0\n",
        "        self.total_words_processed = 0\n",
        "\n",
        "        # Performance tracking\n",
        "        self.processing_times = []\n",
        "        self.buffer_states = []\n",
        "\n",
        "        print(f\"üîÑ StreamingBuffer initialized:\")\n",
        "        print(f\"   Buffer size: {buffer_size} words\")\n",
        "        print(f\"   Overlap frames: {overlap_frames} words\")\n",
        "\n",
        "    def add_word(self, word: str) -> bool:\n",
        "        \"\"\"Add word to buffer and return True if ready for processing\"\"\"\n",
        "        self.text_buffer.append(word.strip())\n",
        "        ready = len(self.text_buffer) >= self.buffer_size\n",
        "\n",
        "        # Track buffer state\n",
        "        self.buffer_states.append({\n",
        "            'buffer_length': len(self.text_buffer),\n",
        "            'word_added': word,\n",
        "            'ready_for_processing': ready\n",
        "        })\n",
        "\n",
        "        return ready\n",
        "\n",
        "    def get_processing_chunk(self) -> List[str]:\n",
        "        \"\"\"Get chunk for processing with intelligent overlap management\"\"\"\n",
        "        if len(self.text_buffer) < self.buffer_size:\n",
        "            return []\n",
        "\n",
        "        # Get chunk with overlap for smoother transitions\n",
        "        chunk_size = min(self.buffer_size + self.overlap_frames, len(self.text_buffer))\n",
        "        chunk = self.text_buffer[:chunk_size]\n",
        "\n",
        "        # Remove processed words but keep overlap\n",
        "        words_to_remove = max(1, self.buffer_size - self.overlap_frames)\n",
        "        self.text_buffer = self.text_buffer[words_to_remove:]\n",
        "\n",
        "        self.processed_count += 1\n",
        "        self.total_words_processed += words_to_remove\n",
        "\n",
        "        return chunk\n",
        "\n",
        "    def get_remaining_words(self) -> List[str]:\n",
        "        \"\"\"Get any remaining words in buffer\"\"\"\n",
        "        remaining = self.text_buffer.copy()\n",
        "        self.text_buffer.clear()\n",
        "        return remaining\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset buffer state\"\"\"\n",
        "        self.text_buffer.clear()\n",
        "        self.processed_count = 0\n",
        "        self.total_words_processed = 0\n",
        "        self.processing_times.clear()\n",
        "        self.buffer_states.clear()\n",
        "\n",
        "        # Clear audio queue\n",
        "        while not self.audio_buffer.empty():\n",
        "            try:\n",
        "                self.audio_buffer.get_nowait()\n",
        "            except queue.Empty:\n",
        "                break\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Get buffer performance statistics\"\"\"\n",
        "        return {\n",
        "            'total_chunks_processed': self.processed_count,\n",
        "            'total_words_processed': self.total_words_processed,\n",
        "            'current_buffer_size': len(self.text_buffer),\n",
        "            'avg_processing_time': np.mean(self.processing_times) if self.processing_times else 0,\n",
        "            'buffer_efficiency': self.total_words_processed / max(1, self.processed_count * self.buffer_size)\n",
        "        }\n",
        "\n",
        "# ==============================================================================\n",
        "# CELL 8: Causal Convolution and Attention Components\n",
        "# ==============================================================================\n",
        "\n",
        "class CausalConv1d(nn.Module):\n",
        "    \"\"\"1D causal convolution for streaming with no future information leakage\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int,\n",
        "                 kernel_size: int, dilation: int = 1, groups: int = 1):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dilation = dilation\n",
        "        self.padding = (kernel_size - 1) * dilation\n",
        "\n",
        "        self.conv = nn.Conv1d(\n",
        "            in_channels, out_channels, kernel_size,\n",
        "            dilation=dilation, padding=self.padding, groups=groups\n",
        "        )\n",
        "\n",
        "        # Initialize weights for stable training\n",
        "        nn.init.kaiming_normal_(self.conv.weight, mode='fan_out', nonlinearity='relu')\n",
        "        if self.conv.bias is not None:\n",
        "            nn.init.zeros_(self.conv.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass ensuring causality\"\"\"\n",
        "        # Apply convolution with padding\n",
        "        x = self.conv(x)\n",
        "\n",
        "        # Remove future information by cropping the end\n",
        "        if self.padding > 0:\n",
        "            x = x[:, :, :-self.padding]\n",
        "\n",
        "        return x\n",
        "\n",
        "class DepthwiseSeparableConv1d(nn.Module):\n",
        "    \"\"\"Efficient depthwise separable convolution for reduced parameters\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, dilation: int = 1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Depthwise convolution\n",
        "        self.depthwise = CausalConv1d(\n",
        "            in_channels, in_channels, kernel_size,\n",
        "            dilation=dilation, groups=in_channels\n",
        "        )\n",
        "\n",
        "        # Pointwise convolution\n",
        "        self.pointwise = nn.Conv1d(in_channels, out_channels, 1)\n",
        "\n",
        "        # Normalization and activation\n",
        "        self.norm = nn.BatchNorm1d(out_channels)\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        x = self.norm(x)\n",
        "        return self.activation(x)\n",
        "\n",
        "class MultiHeadCausalAttention(nn.Module):\n",
        "    \"\"\"Multi-head causal attention optimized for streaming TTS\"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim: int, num_heads: int, dropout: float = 0.1,\n",
        "                 use_relative_position: bool = True):\n",
        "        super().__init__()\n",
        "        assert hidden_dim % num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        self.use_relative_position = use_relative_position\n",
        "\n",
        "        # Linear projections for Q, K, V\n",
        "        self.q_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
        "        self.k_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
        "        self.v_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
        "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        # Relative positional encoding\n",
        "        if use_relative_position:\n",
        "            self.max_relative_position = 32\n",
        "            self.relative_position_k = nn.Parameter(\n",
        "                torch.randn(2 * self.max_relative_position + 1, self.head_dim)\n",
        "            )\n",
        "            self.relative_position_v = nn.Parameter(\n",
        "                torch.randn(2 * self.max_relative_position + 1, self.head_dim)\n",
        "            )\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize attention weights\"\"\"\n",
        "        for module in [self.q_proj, self.k_proj, self.v_proj]:\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
        "        nn.init.zeros_(self.out_proj.bias)\n",
        "\n",
        "        if self.use_relative_position:\n",
        "            nn.init.xavier_uniform_(self.relative_position_k)\n",
        "            nn.init.xavier_uniform_(self.relative_position_v)\n",
        "\n",
        "    def _get_relative_positions(self, seq_len: int) -> torch.Tensor:\n",
        "        \"\"\"Get relative position indices\"\"\"\n",
        "        positions = torch.arange(seq_len, dtype=torch.long)\n",
        "        relative_positions = positions[:, None] - positions[None, :]\n",
        "        relative_positions = torch.clamp(\n",
        "            relative_positions,\n",
        "            -self.max_relative_position,\n",
        "            self.max_relative_position\n",
        "        ) + self.max_relative_position\n",
        "        return relative_positions\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None,\n",
        "                context: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass with optional context for streaming\n",
        "        Args:\n",
        "            x: Input tensor [batch, seq_len, hidden_dim]\n",
        "            mask: Attention mask [seq_len, seq_len]\n",
        "            context: Previous context for streaming [batch, context_len, hidden_dim]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Combine with context if provided\n",
        "        if context is not None:\n",
        "            kv_input = torch.cat([context, x], dim=1)\n",
        "            context_len = context.size(1)\n",
        "        else:\n",
        "            kv_input = x\n",
        "            context_len = 0\n",
        "\n",
        "        kv_seq_len = kv_input.size(1)\n",
        "\n",
        "        # Project to Q, K, V\n",
        "        q = self.q_proj(x)  # Only query from current input\n",
        "        k = self.k_proj(kv_input)  # Key from input + context\n",
        "        v = self.v_proj(kv_input)  # Value from input + context\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(batch_size, kv_seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(batch_size, kv_seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Compute attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        # Add relative position bias if enabled\n",
        "        if self.use_relative_position:\n",
        "            rel_pos_indices = self._get_relative_positions(seq_len).to(x.device)\n",
        "            rel_pos_k = self.relative_position_k[rel_pos_indices]  # [seq_len, seq_len, head_dim]\n",
        "            rel_scores = torch.einsum('bhid,ijd->bhij', q, rel_pos_k)\n",
        "            scores = scores + rel_scores\n",
        "\n",
        "        # Apply causal mask\n",
        "        if mask is not None:\n",
        "            # Expand mask for context if needed\n",
        "            if context is not None:\n",
        "                expanded_mask = torch.zeros(seq_len, kv_seq_len, dtype=mask.dtype, device=mask.device)\n",
        "                expanded_mask[:, context_len:] = mask\n",
        "                mask = expanded_mask\n",
        "            scores = scores.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
        "\n",
        "        # Apply attention\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.attn_dropout(attn_weights)\n",
        "\n",
        "        # Apply attention to values\n",
        "        attn_output = torch.matmul(attn_weights, v)\n",
        "\n",
        "        # Add relative position bias to values if enabled\n",
        "        if self.use_relative_position:\n",
        "            rel_pos_v = self.relative_position_v[rel_pos_indices]  # [seq_len, seq_len, head_dim]\n",
        "            rel_attn = torch.einsum('bhij,ijd->bhid', attn_weights, rel_pos_v)\n",
        "            attn_output = attn_output + rel_attn\n",
        "\n",
        "        # Reshape and project output\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
        "            batch_size, seq_len, self.hidden_dim\n",
        "        )\n",
        "\n",
        "        output = self.out_proj(attn_output)\n",
        "        return self.dropout(output)\n",
        "\n",
        "# ==============================================================================\n",
        "# CELL 9: Streaming Transformer Layers\n",
        "# ==============================================================================\n",
        "\n",
        "class StreamingTransformerLayer(nn.Module):\n",
        "    \"\"\"Transformer layer optimized for streaming with context management\"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim: int, num_heads: int, ff_dim: Optional[int] = None,\n",
        "                 dropout: float = 0.1, use_relative_position: bool = True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        ff_dim = ff_dim or hidden_dim * 4\n",
        "\n",
        "        # Multi-head causal attention\n",
        "        self.self_attention = MultiHeadCausalAttention(\n",
        "            hidden_dim, num_heads, dropout, use_relative_position\n",
        "        )\n",
        "\n",
        "        # Feed-forward network with GELU activation\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, ff_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_dim, hidden_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Layer normalization (Pre-LN for better training stability)\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize layer weights\"\"\"\n",
        "        for module in self.feed_forward:\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, causal: bool = True,\n",
        "                context: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass with optional streaming context\n",
        "        Args:\n",
        "            x: Input tensor [batch, seq_len, hidden_dim]\n",
        "            causal: Whether to apply causal masking\n",
        "            context: Previous context for streaming\n",
        "        \"\"\"\n",
        "        seq_len = x.size(1)\n",
        "\n",
        "        # Create causal mask if needed\n",
        "        mask = None\n",
        "        if causal:\n",
        "            # Create lower triangular mask for causality\n",
        "            mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
        "            mask = mask.to(x.device)\n",
        "\n",
        "        # Pre-LN: Layer norm before attention\n",
        "        normed_x = self.norm1(x)\n",
        "\n",
        "        # Self-attention with residual connection\n",
        "        attn_output = self.self_attention(normed_x, mask=mask, context=context)\n",
        "        x = x + attn_output\n",
        "\n",
        "        # Pre-LN: Layer norm before feed-forward\n",
        "        normed_x = self.norm2(x)\n",
        "\n",
        "        # Feed-forward with residual connection\n",
        "        ff_output = self.feed_forward(normed_x)\n",
        "        x = x + ff_output\n",
        "\n",
        "        return x\n",
        "\n",
        "class StreamingTransformerEncoder(nn.Module):\n",
        "    \"\"\"Multi-layer streaming transformer encoder with context management\"\"\"\n",
        "\n",
        "    def __init__(self, num_layers: int, hidden_dim: int, num_heads: int,\n",
        "                 ff_dim: Optional[int] = None, dropout: float = 0.1,\n",
        "                 use_relative_position: bool = True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.context_length = 2  # Number of tokens to keep as context\n",
        "\n",
        "        # Transformer layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            StreamingTransformerLayer(\n",
        "                hidden_dim, num_heads, ff_dim, dropout, use_relative_position\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Final layer norm\n",
        "        self.final_norm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        print(f\"üèóÔ∏è StreamingTransformerEncoder:\")\n",
        "        print(f\"   Layers: {num_layers}\")\n",
        "        print(f\"   Hidden dim: {hidden_dim}\")\n",
        "        print(f\"   Attention heads: {num_heads}\")\n",
        "        print(f\"   Context length: {self.context_length}\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor, causal: bool = True,\n",
        "                context: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass through all transformer layers\n",
        "        Returns:\n",
        "            output: Transformed representation\n",
        "            new_context: Context for next streaming chunk\n",
        "        \"\"\"\n",
        "        # Apply all transformer layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, causal=causal, context=context)\n",
        "            # Update context for next layer (optional improvement)\n",
        "            if context is not None and x.size(1) >= self.context_length:\n",
        "                context = x[:, -self.context_length:, :].detach()\n",
        "\n",
        "        # Apply final normalization\n",
        "        x = self.final_norm(x)\n",
        "\n",
        "        # Extract new context for streaming\n",
        "        new_context = None\n",
        "        if x.size(1) >= self.context_length:\n",
        "            new_context = x[:, -self.context_length:, :].detach()\n",
        "\n",
        "        return x, new_context\n",
        "\n",
        "# ==============================================================================\n",
        "# CELL 10: Duration Prediction and Alignment\n",
        "# ==============================================================================\n",
        "\n",
        "class DurationPredictor(nn.Module):\n",
        "    \"\"\"Advanced duration predictor for text-to-speech alignment\"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim: int, filter_size: int = 256, kernel_size: int = 3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.filter_size = filter_size\n",
        "\n",
        "        # Convolutional layers for duration prediction\n",
        "        self.conv_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(hidden_dim, filter_size, kernel_size, padding=kernel_size//2),\n",
        "                nn.GELU(),\n",
        "                nn.BatchNorm1d(filter_size),\n",
        "                nn.Dropout(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(filter_size, filter_size, kernel_size, padding=kernel_size//2),\n",
        "                nn.GELU(),\n",
        "                nn.BatchNorm1d(filter_size),\n",
        "                nn.Dropout(0.1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(filter_size, filter_size // 2, kernel_size, padding=kernel_size//2),\n",
        "                nn.GELU(),\n",
        "                nn.BatchNorm1d(filter_size // 2)\n",
        "            )\n",
        "        ])\n",
        "\n",
        "        # Output projection\n",
        "        self.output_projection = nn.Sequential(\n",
        "            nn.Linear(filter_size // 2, filter_size // 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(filter_size // 4, 1)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize weights for stable training\"\"\"\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Conv1d):\n",
        "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if module.bias is not None:\n",
        "                    nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Predict duration for each token\n",
        "        Args:\n",
        "            x: Hidden representation [batch, seq_len, hidden_dim]\n",
        "        Returns:\n",
        "            durations: Predicted durations [batch, seq_len]\n",
        "        \"\"\"\n",
        "        # Transpose for conv1d: [batch, hidden_dim, seq_len]\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        # Apply convolutional layers\n",
        "        for conv_layer in self.conv_layers:\n",
        "            residual = x\n",
        "            x = conv_layer(x)\n",
        "\n",
        "            # Residual connection if dimensions match\n",
        "            if residual.size(1) == x.size(1):\n",
        "                x = x + residual\n",
        "\n",
        "        # Transpose back: [batch, seq_len, filter_size//2]\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        # Project to duration\n",
        "        durations = self.output_projection(x).squeeze(-1)\n",
        "\n",
        "        # Ensure positive durations with reasonable range for LibriSpeech\n",
        "        # Range: 1 to 25 frames (suitable for 16kHz, 200 hop_length)\n",
        "        durations = torch.sigmoid(durations) * 24 + 1\n",
        "\n",
        "        return durations\n",
        "\n",
        "class LengthRegulator(nn.Module):\n",
        "    \"\"\"Advanced length regulator for duration-based alignment\"\"\"\n",
        "\n",
        "    def __init__(self, max_duration: int = 100):\n",
        "        super().__init__()\n",
        "        self.max_duration = max_duration\n",
        "\n",
        "    def forward(self, hidden: torch.Tensor, durations: torch.Tensor,\n",
        "                target_length: Optional[int] = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Regulate sequence length based on predicted durations\n",
        "        Args:\n",
        "            hidden: Hidden representations [batch, seq_len, hidden_dim]\n",
        "            durations: Duration for each token [batch, seq_len]\n",
        "            target_length: Optional target length for training\n",
        "        Returns:\n",
        "            expanded: Length-regulated sequence\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, hidden_dim = hidden.shape\n",
        "\n",
        "        # Clamp durations to reasonable bounds\n",
        "        durations = torch.clamp(durations, min=0.5, max=self.max_duration)\n",
        "\n",
        "        # If target length is provided (training), scale durations\n",
        "        if target_length is not None:\n",
        "            total_duration = durations.sum(dim=1, keepdim=True)\n",
        "            scale_factor = target_length / (total_duration + 1e-8)\n",
        "            durations = durations * scale_factor\n",
        "\n",
        "        # Round to integers\n",
        "        durations = torch.round(durations).long()\n",
        "        durations = torch.clamp(durations, min=1, max=self.max_duration)\n",
        "\n",
        "        # Calculate output length\n",
        "        max_len = min(durations.sum(dim=1).max().item(), 2000)  # Cap for memory\n",
        "\n",
        "        # Expand sequences\n",
        "        expanded = torch.zeros(batch_size, max_len, hidden_dim,\n",
        "                              device=hidden.device, dtype=hidden.dtype)\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            pos = 0\n",
        "            for i in range(seq_len):\n",
        "                if pos >= max_len:\n",
        "                    break\n",
        "\n",
        "                dur = min(durations[b, i].item(), max_len - pos)\n",
        "                if dur > 0:\n",
        "                    # Repeat hidden state for duration\n",
        "                    expanded[b, pos:pos+dur, :] = hidden[b, i, :].unsqueeze(0).repeat(dur, 1)\n",
        "                    pos += dur\n",
        "\n",
        "        return expanded\n",
        "\n",
        "# ==============================================================================\n",
        "# CELL 11: Mel-Spectrogram Decoder\n",
        "# ==============================================================================\n",
        "\n",
        "class StreamingMelDecoder(nn.Module):\n",
        "    \"\"\"Advanced mel-spectrogram decoder with streaming capability\"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim: int, mel_dim: int, prenet_dim: int = 256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.mel_dim = mel_dim\n",
        "        self.prenet_dim = prenet_dim\n",
        "\n",
        "        # Pre-net for input conditioning\n",
        "        self.prenet = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, prenet_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(prenet_dim, prenet_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(prenet_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Causal convolution stack for streaming\n",
        "        self.causal_convs = nn.ModuleList([\n",
        "            CausalConv1d(hidden_dim, hidden_dim, kernel_size=5, dilation=1),\n",
        "            CausalConv1d(hidden_dim, hidden_dim, kernel_size=5, dilation=2),\n",
        "            CausalConv1d(hidden_dim, hidden_dim, kernel_size=5, dilation=4),\n",
        "            CausalConv1d(hidden_dim, hidden_dim, kernel_size=5, dilation=8),\n",
        "        ])\n",
        "\n",
        "        # Normalization for each conv layer\n",
        "        self.conv_norms = nn.ModuleList([\n",
        "            nn.LayerNorm(hidden_dim) for _ in range(len(self.causal_convs))\n",
        "        ])\n",
        "\n",
        "        # Post-processing network\n",
        "        self.postnet = nn.Sequential(\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.GELU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.GELU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.GELU(),\n",
        "            nn.BatchNorm1d(hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Output projection to mel-spectrogram\n",
        "        self.mel_projection = nn.Linear(hidden_dim, mel_dim)\n",
        "\n",
        "        # Length regulator\n",
        "        self.length_regulator = LengthRegulator()\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "        print(f\"üéº StreamingMelDecoder:\")\n",
        "        print(f\"   Hidden dim: {hidden_dim}\")\n",
        "        print(f\"   Mel dim: {mel_dim}\")\n",
        "        print(f\"   Prenet dim: {prenet_dim}\")\n",
        "        print(f\"   Causal conv layers: {len(self.causal_convs)}\")\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize decoder weights\"\"\"\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.Conv1d):\n",
        "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if module.bias is not None:\n",
        "                    nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, hidden: torch.Tensor, durations: torch.Tensor,\n",
        "                target_length: Optional[int] = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Generate mel-spectrogram from hidden representations\n",
        "        Args:\n",
        "            hidden: Text encoder output [batch, seq_len, hidden_dim]\n",
        "            durations: Duration predictions [batch, seq_len]\n",
        "            target_length: Target mel length for training\n",
        "        Returns:\n",
        "            mel_output: Generated mel-spectrogram [batch, mel_len, mel_dim]\n",
        "        \"\"\"\n",
        "        # Apply pre-net\n",
        "        hidden = self.prenet(hidden)\n",
        "\n",
        "        # Length regulation based on durations\n",
        "        hidden = self.length_regulator(hidden, durations, target_length)\n",
        "\n",
        "        # Apply causal convolutions with residual connections\n",
        "        hidden_conv = hidden.transpose(1, 2)  # [batch, hidden_dim, seq_len]\n",
        "\n",
        "        for conv, norm in zip(self.causal_convs, self.conv_norms):\n",
        "            residual = hidden_conv\n",
        "            hidden_conv = conv(hidden_conv)\n",
        "            hidden_conv = hidden_conv + residual  # Residual connection\n",
        "\n",
        "            # Apply layer normalization\n",
        "            hidden_conv = norm(hidden_conv.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "        # Apply post-processing with residual\n",
        "        post_processed = self.postnet(hidden_conv)\n",
        "        hidden_conv = hidden_conv + post_processed\n",
        "\n",
        "        # Convert back to [batch, seq_len, hidden_dim]\n",
        "        hidden = hidden_conv.transpose(1, 2)\n",
        "\n",
        "        # Project to mel-spectrogram\n",
        "        mel_output = self.mel_projection(hidden)\n",
        "\n",
        "        return mel_output\n",
        "\n",
        "# ==============================================================================\n",
        "# CELL 12: Advanced Vocoder Architecture\n",
        "# ==============================================================================\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Enhanced residual block with multiple kernel sizes\"\"\"\n",
        "\n",
        "    def __init__(self, channels: int, kernel_sizes: List[int] = [3, 5, 7]):\n",
        "        super().__init__()\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        for kernel_size in kernel_sizes:\n",
        "            self.convs.append(nn.Sequential(\n",
        "                nn.Conv1d(channels, channels, kernel_size, padding=kernel_size//2),\n",
        "                nn.BatchNorm1d(channels),\n",
        "                nn.LeakyReLU(0.2)\n",
        "            ))\n",
        "\n",
        "        self.fusion_conv = nn.Conv1d(channels * len(kernel_sizes), channels, 1)\n",
        "        self.norm = nn.BatchNorm1d(channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        residual = x\n",
        "\n",
        "        # Apply multiple kernel sizes\n",
        "        conv_outputs = []\n",
        "        for conv in self.convs:\n",
        "            conv_outputs.append(conv(x))\n",
        "\n",
        "        # Concatenate and fuse\n",
        "        fused = torch.cat(conv_outputs, dim=1)\n",
        "        fused = self.fusion_conv(fused)\n",
        "        fused = self.norm(fused)\n",
        "\n",
        "        return F.leaky_relu(fused + residual, 0.2)\n",
        "\n",
        "class LibriSpeechVocoder(nn.Module):\n",
        "    \"\"\"Advanced vocoder optimized for LibriSpeech 16kHz with high quality output\"\"\"\n",
        "\n",
        "    def __init__(self, mel_dim: int, hop_length: int = 200,\n",
        "                 upsample_rates: List[int] = None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mel_dim = mel_dim\n",
        "        self.hop_length = hop_length\n",
        "\n",
        "        # Default upsampling rates for 16kHz, 200 hop_length\n",
        "        if upsample_rates is None:\n",
        "            upsample_rates = [2, 2, 5, 5, 2]  # Total: 200x upsampling\n",
        "\n",
        "        self.upsample_rates = upsample_rates\n",
        "\n",
        "        # Input projection\n",
        "        self.input_proj = nn.Conv1d(mel_dim, 512, 7, padding=3)\n",
        "\n",
        "        # Upsampling layers with decreasing channels\n",
        "        channels = [512, 256, 128, 64, 32]\n",
        "        self.upsample_layers = nn.ModuleList()\n",
        "\n",
        "        for i, rate in enumerate(upsample_rates):\n",
        "            in_ch = channels[i] if i < len(channels) else 32\n",
        "            out_ch = channels[i+1] if i+1 < len(channels) else 32\n",
        "\n",
        "            self.upsample_layers.append(nn.Sequential(\n",
        "                nn.ConvTranspose1d(in_ch, out_ch, rate*2, rate, rate//2),\n",
        "                nn.BatchNorm1d(out_ch),\n",
        "                nn.LeakyReLU(0.2)\n",
        "            ))\n",
        "\n",
        "        # Residual blocks for each upsampling stage\n",
        "        self.res_blocks = nn.ModuleList([\n",
        "            ResidualBlock(ch) for ch in channels[1:]\n",
        "        ])\n",
        "\n",
        "        # Multi-receptive field fusion\n",
        "        self.mrf_blocks = nn.ModuleList([\n",
        "            nn.Conv1d(32, 32, 3, padding=1),\n",
        "            nn.Conv1d(32, 32, 5, padding=2),\n",
        "            nn.Conv1d(32, 32, 7, padding=3),\n",
        "        ])\n",
        "\n",
        "        # Output layers\n",
        "        self.output_conv = nn.Sequential(\n",
        "            nn.Conv1d(32, 16, 7, padding=3),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv1d(16, 8, 7, padding=3),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv1d(8, 1, 7, padding=3),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "        print(f\"üîä LibriSpeechVocoder:\")\n",
        "        print(f\"   Mel dim: {mel_dim}\")\n",
        "        print(f\"   Hop length: {hop_length}\")\n",
        "        print(f\"   Upsample rates: {upsample_rates}\")\n",
        "        print(f\"   Total upsampling: {np.prod(upsample_rates)}x\")\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize vocoder weights\"\"\"\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, (nn.Conv1d, nn.ConvTranspose1d)):\n",
        "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
        "                if module.bias is not None:\n",
        "                    nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, mel: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Convert mel-spectrogram to waveform\n",
        "        Args:\n",
        "            mel: Mel-spectrogram [batch, mel_len, mel_dim]\n",
        "        Returns:\n",
        "            waveform: Generated audio [batch, audio_len]\n",
        "        \"\"\"\n",
        "        # Transpose to [batch, mel_dim, mel_len] for convolution\n",
        "        x = mel.transpose(1, 2)\n",
        "\n",
        "        # Input projection\n",
        "        x = self.input_proj(x)\n",
        "\n",
        "        # Upsampling with residual blocks\n",
        "        for upsample, res_block in zip(self.upsample_layers, self.res_blocks):\n",
        "            x = upsample(x)\n",
        "            x = res_block(x)\n",
        "\n",
        "        # Multi-receptive field fusion\n",
        "        mrf_outputs = []\n",
        "        for mrf_conv in self.mrf_blocks:\n",
        "            mrf_outputs.append(mrf_conv(x))\n",
        "\n",
        "        # Average fusion of different receptive fields\n",
        "        x = sum(mrf_outputs) / len(mrf_outputs)\n",
        "\n",
        "        # Generate final waveform\n",
        "        waveform = self.output_conv(x)\n",
        "\n",
        "        return waveform.squeeze(1)  # [batch, audio_len]\n",
        "\n",
        "# ==============================================================================\n",
        "# CELL 13: Complete Streaming TTS Model\n",
        "# ==============================================================================\n",
        "\n",
        "class StreamingTTSModel(nn.Module):\n",
        "    \"\"\"Complete streaming TTS model with all components integrated\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, hidden_dim: int = 512,\n",
        "                 num_heads: int = 8, num_layers: int = 6,\n",
        "                 mel_dim: int = 80, max_seq_len: int = 500,\n",
        "                 dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Model configuration\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.mel_dim = mel_dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Text embedding layer\n",
        "        self.text_embedding = nn.Embedding(vocab_size, hidden_dim, padding_idx=0)\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoding = self._create_positional_encoding(max_seq_len, hidden_dim)\n",
        "        self.register_buffer('pos_encoding_buffer', self.pos_encoding)\n",
        "\n",
        "        # Streaming transformer encoder\n",
        "        self.encoder = StreamingTransformerEncoder(\n",
        "            num_layers=num_layers,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            use_relative_position=True\n",
        "        )\n",
        "\n",
        "        # Duration predictor\n",
        "        self.duration_predictor = DurationPredictor(hidden_dim)\n",
        "\n",
        "        # Mel-spectrogram decoder\n",
        "        self.mel_decoder = StreamingMelDecoder(hidden_dim, mel_dim)\n",
        "\n",
        "        # Vocoder for audio synthesis\n",
        "        self.vocoder = LibriSpeechVocoder(mel_dim, hop_length=200)  # From config\n",
        "\n",
        "        # Streaming state management\n",
        "        self.streaming_context = None\n",
        "        self.streaming_position = 0\n",
        "\n",
        "        # Initialize model weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "        # Calculate model size\n",
        "        total_params = sum(p.numel() for p in self.parameters())\n",
        "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "        print(f\"üß† StreamingTTSModel Complete:\")\n",
        "        print(f\"   Vocabulary size: {vocab_size:,}\")\n",
        "        print(f\"   Hidden dimension: {hidden_dim}\")\n",
        "        print(f\"   Transformer layers: {num_layers}\")\n",
        "        print(f\"   Attention heads: {num_heads}\")\n",
        "        print(f\"   Mel dimensions: {mel_dim}\")\n",
        "        print(f\"   Total parameters: {total_params:,}\")\n",
        "        print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "        print(f\"   Model size: {total_params * 4 / 1024**2:.1f} MB\")\n",
        "\n",
        "    def _create_positional_encoding(self, max_len: int, d_model: int) -> torch.Tensor:\n",
        "        \"\"\"Create sinusoidal positional encoding\"\"\"\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                           (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        return pe.unsqueeze(0)  # [1, max_len, d_model]\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize all model weights\"\"\"\n",
        "        # Text embedding\n",
        "        nn.init.normal_(self.text_embedding.weight, 0, 0.1)\n",
        "        if hasattr(self.text_embedding, 'padding_idx') and self.text_embedding.padding_idx is not None:\n",
        "            with torch.no_grad():\n",
        "                self.text_embedding.weight[self.text_embedding.padding_idx].fill_(0)\n",
        "\n",
        "        # Apply initialization to all modules\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                if module.bias is not None:\n",
        "                    nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, (nn.BatchNorm1d, nn.LayerNorm)):\n",
        "                if hasattr(module, 'weight') and module.weight is not None:\n",
        "                    nn.init.ones_(module.weight)\n",
        "                if hasattr(module, 'bias') and module.bias is not None:\n",
        "                    nn.init.zeros_(module.bias)\n",
        "\n",
        "    def reset_streaming_state(self):\n",
        "        \"\"\"Reset streaming state for new session\"\"\"\n",
        "        self.streaming_context = None\n",
        "        self.streaming_position = 0\n",
        "\n",
        "    def forward_streaming(self, text_ids: torch.Tensor,\n",
        "                         return_alignments: bool = False) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Streaming forward pass for real-time synthesis\n",
        "        Args:\n",
        "            text_ids: Token IDs [batch, seq_len]\n",
        "            return_alignments: Whether to return duration predictions\n",
        "        Returns:\n",
        "            mel_output: Generated mel-spectrogram\n",
        "            durations: Duration predictions (if return_alignments=True)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = text_ids.shape\n",
        "\n",
        "        # Text embedding\n",
        "        text_emb = self.text_embedding(text_ids)\n",
        "        text_emb = self.embedding_dropout(text_emb)\n",
        "\n",
        "        # Add positional encoding with streaming position\n",
        "        pos_start = min(self.streaming_position, self.max_seq_len - seq_len)\n",
        "        pos_end = min(pos_start + seq_len, self.max_seq_len)\n",
        "\n",
        "        if pos_end > pos_start:\n",
        "            pos_enc = self.pos_encoding_buffer[:, pos_start:pos_end, :]\n",
        "            # Handle case where we need fewer positions than requested\n",
        "            if pos_enc.size(1) < seq_len:\n",
        "                # Pad with the last positional encoding\n",
        "                last_pos = self.pos_encoding_buffer[:, -1:, :]\n",
        "                padding_needed = seq_len - pos_enc.size(1)\n",
        "                padding = last_pos.repeat(1, padding_needed, 1)\n",
        "                pos_enc = torch.cat([pos_enc, padding], dim=1)\n",
        "\n",
        "            text_emb = text_emb + pos_enc.to(text_emb.device)\n",
        "\n",
        "        # Transformer encoding with streaming context\n",
        "        hidden, new_context = self.encoder(\n",
        "            text_emb,\n",
        "            causal=True,\n",
        "            context=self.streaming_context\n",
        "        )\n",
        "\n",
        "        # Update streaming state\n",
        "        self.streaming_context = new_context\n",
        "        self.streaming_position += max(1, seq_len - self.encoder.context_length)\n",
        "\n",
        "        # Duration prediction\n",
        "        durations = self.duration_predictor(hidden)\n",
        "\n",
        "        # Mel-spectrogram generation\n",
        "        mel_output = self.mel_decoder(hidden, durations)\n",
        "\n",
        "        if return_alignments:\n",
        "            return mel_output, durations\n",
        "        return mel_output\n",
        "\n",
        "    def synthesize_streaming(self, mel_spec: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Convert mel-spectrogram to audio waveform\"\"\"\n",
        "        return self.vocoder(mel_spec)\n",
        "\n",
        "    def forward(self, text_ids: torch.Tensor,\n",
        "                target_mel_length: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Standard forward pass for training\n",
        "        Args:\n",
        "            text_ids: Token IDs [batch, seq_len]\n",
        "            target_mel_length: Target mel length for training alignment\n",
        "        Returns:\n",
        "            mel_output: Generated mel-spectrogram\n",
        "            durations: Predicted durations\n",
        "        \"\"\"\n",
        "        # Reset streaming state for non-streaming forward pass\n",
        "        self.reset_streaming_state()\n",
        "        return self.forward_streaming(text_ids, return_alignments=True)\n",
        "\n",
        "    def inference(self, text_ids: torch.Tensor,\n",
        "                  return_intermediate: bool = False) -> dict:\n",
        "        \"\"\"\n",
        "        Complete inference pipeline\n",
        "        Args:\n",
        "            text_ids: Token IDs [batch, seq_len]\n",
        "            return_intermediate: Whether to return intermediate outputs\n",
        "        Returns:\n",
        "            Dictionary with outputs and optional intermediate results\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Generate mel-spectrogram\n",
        "            mel_output, durations = self.forward(text_ids)\n",
        "\n",
        "            # Generate audio\n",
        "            audio_output = self.synthesize_streaming(mel_output)\n",
        "\n",
        "            results = {\n",
        "                'audio': audio_output,\n",
        "                'mel_spectrogram': mel_output,\n",
        "                'durations': durations\n",
        "            }\n",
        "\n",
        "            if return_intermediate:\n",
        "                # Add intermediate representations for analysis\n",
        "                text_emb = self.text_embedding(text_ids)\n",
        "                hidden, _ = self.encoder(text_emb, causal=False)\n",
        "\n",
        "                results.update({\n",
        "                    'text_embedding': text_emb,\n",
        "                    'encoder_output': hidden,\n",
        "                    'text_length': text_ids.size(1),\n",
        "                    'mel_length': mel_output.size(1),\n",
        "                    'audio_length': audio_output.size(-1)\n",
        "                })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def get_model_stats(self) -> dict:\n",
        "        \"\"\"Get comprehensive model statistics\"\"\"\n",
        "        total_params = sum(p.numel() for p in self.parameters())\n",
        "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "        component_params = {}\n",
        "        for name, module in self.named_children():\n",
        "            component_params[name] = sum(p.numel() for p in module.parameters())\n",
        "\n",
        "        return {\n",
        "            'total_parameters': total_params,\n",
        "            'trainable_parameters': trainable_params,\n",
        "            'model_size_mb': total_params * 4 / 1024**2,\n",
        "            'component_parameters': component_params,\n",
        "            'vocab_size': self.vocab_size,\n",
        "            'hidden_dim': self.hidden_dim,\n",
        "            'num_layers': self.num_layers,\n",
        "            'mel_dim': self.mel_dim\n",
        "        }\n",
        "\n",
        "# ==============================================================================\n",
        "# CELL 14: Model Testing and Validation\n",
        "# ==============================================================================\n",
        "\n",
        "def test_model_components():\n",
        "    \"\"\"Test individual model components\"\"\"\n",
        "    print(\"üß™ Testing Model Components\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Test parameters (assuming Part 1 components are available)\n",
        "    try:\n",
        "        from config import config  # From Part 1\n",
        "        vocab_size = getattr(config, 'VOCAB_SIZE', 1000)\n",
        "        hidden_dim = getattr(config, 'HIDDEN_DIM', 512)\n",
        "        mel_dim = getattr(config, 'MEL_DIM', 80)\n",
        "        device = getattr(config, 'DEVICE', torch.device('cpu'))\n",
        "    except:\n",
        "        # Fallback values if config not available\n",
        "        vocab_size = 1000\n",
        "        hidden_dim = 512\n",
        "        mel_dim = 80\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    batch_size = 2\n",
        "    seq_len = 10\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Test parameters: batch={batch_size}, seq_len={seq_len}\")\n",
        "\n",
        "    # Test StreamingBuffer\n",
        "    print(\"\\nüîÑ Testing StreamingBuffer...\")\n",
        "    buffer = StreamingBuffer(buffer_size=4, overlap_frames=2)\n",
        "    test_words = [\"hello\", \"world\", \"this\", \"is\", \"a\", \"test\"]\n",
        "\n",
        "    for word in test_words:\n",
        "        ready = buffer.add_word(word)\n",
        "        if ready:\n",
        "            chunk = buffer.get_processing_chunk()\n",
        "            print(f\"   Processed chunk: {chunk}\")\n",
        "\n",
        "    print(f\"   Buffer stats: {buffer.get_stats()}\")\n",
        "\n",
        "    # Test MultiHeadCausalAttention\n",
        "    print(\"\\nüîç Testing MultiHeadCausalAttention...\")\n",
        "    attention = MultiHeadCausalAttention(hidden_dim, num_heads=8).to(device)\n",
        "\n",
        "    test_input = torch.randn(batch_size, seq_len, hidden_dim).to(device)\n",
        "\n",
        "    try:\n",
        "        attn_output = attention(test_input)\n",
        "        print(f\"   Input shape: {test_input.shape}\")\n",
        "        print(f\"   Output shape: {attn_output.shape}\")\n",
        "        print(f\"   ‚úÖ Attention test passed\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Attention test failed: {e}\")\n",
        "\n",
        "    # Test StreamingTransformerLayer\n",
        "    print(\"\\nüèóÔ∏è Testing StreamingTransformerLayer...\")\n",
        "    transformer_layer = StreamingTransformerLayer(hidden_dim, num_heads=8).to(device)\n",
        "\n",
        "    try:\n",
        "        layer_output = transformer_layer(test_input)\n",
        "        print(f\"   Input shape: {test_input.shape}\")\n",
        "        print(f\"   Output shape: {layer_output.shape}\")\n",
        "        print(f\"   ‚úÖ Transformer layer test passed\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Transformer layer test failed: {e}\")\n",
        "\n",
        "    # Test DurationPredictor\n",
        "    print(\"\\n‚è±Ô∏è Testing DurationPredictor...\")\n",
        "    duration_predictor = DurationPredictor(hidden_dim).to(device)\n",
        "\n",
        "    try:\n",
        "        duration_output = duration_predictor(test_input)\n",
        "        print(f\"   Input shape: {test_input.shape}\")\n",
        "        print(f\"   Duration output shape: {duration_output.shape}\")\n",
        "        print(f\"   Duration range: {duration_output.min().item():.2f} - {duration_output.max().item():.2f}\")\n",
        "        print(f\"   ‚úÖ Duration predictor test passed\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Duration predictor test failed: {e}\")\n",
        "\n",
        "    # Test StreamingMelDecoder\n",
        "    print(\"\\nüéº Testing StreamingMelDecoder...\")\n",
        "    mel_decoder = StreamingMelDecoder(hidden_dim, mel_dim).to(device)\n",
        "\n",
        "    try:\n",
        "        durations = torch.ones(batch_size, seq_len).to(device) * 10  # 10 frames per token\n",
        "        mel_output = mel_decoder(test_input, durations)\n",
        "        print(f\"   Input shape: {test_input.shape}\")\n",
        "        print(f\"   Duration shape: {durations.shape}\")\n",
        "        print(f\"   Mel output shape: {mel_output.shape}\")\n",
        "        print(f\"   ‚úÖ Mel decoder test passed\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Mel decoder test failed: {e}\")\n",
        "\n",
        "    # Test LibriSpeechVocoder\n",
        "    print(\"\\nüîä Testing LibriSpeechVocoder...\")\n",
        "    vocoder = LibriSpeechVocoder(mel_dim).to(device)\n",
        "\n",
        "    try:\n",
        "        # Create test mel-spectrogram\n",
        "        mel_len = 100\n",
        "        test_mel = torch.randn(batch_size, mel_len, mel_dim).to(device)\n",
        "\n",
        "        audio_output = vocoder(test_mel)\n",
        "        expected_audio_len = mel_len * 200  # hop_length = 200\n",
        "\n",
        "        print(f\"   Mel input shape: {test_mel.shape}\")\n",
        "        print(f\"   Audio output shape: {audio_output.shape}\")\n",
        "        print(f\"   Expected audio length: ~{expected_audio_len}\")\n",
        "        print(f\"   Actual audio length: {audio_output.size(-1)}\")\n",
        "        print(f\"   ‚úÖ Vocoder test passed\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Vocoder test failed: {e}\")\n",
        "\n",
        "def test_complete_model():\n",
        "    \"\"\"Test the complete streaming TTS model\"\"\"\n",
        "    print(\"\\nüöÄ Testing Complete StreamingTTSModel\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Test parameters\n",
        "    try:\n",
        "        from config import config\n",
        "        vocab_size = getattr(config, 'VOCAB_SIZE', 1000)\n",
        "        hidden_dim = getattr(config, 'HIDDEN_DIM', 512)\n",
        "        device = getattr(config, 'DEVICE', torch.device('cpu'))\n",
        "    except:\n",
        "        vocab_size = 1000\n",
        "        hidden_dim = 512\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Create model\n",
        "    model = StreamingTTSModel(\n",
        "        vocab_size=vocab_size,\n",
        "        hidden_dim=hidden_dim,\n",
        "        num_heads=8,\n",
        "        num_layers=6,\n",
        "        mel_dim=80\n",
        "    ).to(device)\n",
        "\n",
        "    # Test data\n",
        "    batch_size = 2\n",
        "    seq_len = 15\n",
        "    test_text_ids = torch.randint(1, vocab_size, (batch_size, seq_len)).to(device)\n",
        "\n",
        "    print(f\"Model device: {next(model.parameters()).device}\")\n",
        "    print(f\"Test input shape: {test_text_ids.shape}\")\n",
        "\n",
        "    # Test standard forward pass\n",
        "    print(\"\\nüìù Testing standard forward pass...\")\n",
        "    try:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            mel_output, durations = model.forward(test_text_ids)\n",
        "\n",
        "        print(f\"   ‚úÖ Forward pass successful\")\n",
        "        print(f\"   Mel output shape: {mel_output.shape}\")\n",
        "        print(f\"   Duration shape: {durations.shape}\")\n",
        "        print(f\"   Duration stats: min={durations.min():.2f}, max={durations.max():.2f}, mean={durations.mean():.2f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Forward pass failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return\n",
        "\n",
        "    # Test streaming forward pass\n",
        "    print(\"\\nüîÑ Testing streaming forward pass...\")\n",
        "    try:\n",
        "        model.reset_streaming_state()\n",
        "\n",
        "        # Process in chunks\n",
        "        chunk_size = 5\n",
        "        all_mels = []\n",
        "\n",
        "        for i in range(0, seq_len, chunk_size):\n",
        "            chunk = test_text_ids[:, i:i+chunk_size]\n",
        "            if chunk.size(1) > 0:\n",
        "                mel_chunk = model.forward_streaming(chunk)\n",
        "                all_mels.append(mel_chunk)\n",
        "                print(f\"   Chunk {i//chunk_size + 1}: input {chunk.shape} -> mel {mel_chunk.shape}\")\n",
        "\n",
        "        print(f\"   ‚úÖ Streaming forward pass successful\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Streaming forward pass failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    # Test audio synthesis\n",
        "    print(\"\\nüîä Testing audio synthesis...\")\n",
        "    try:\n",
        "        audio_output = model.synthesize_streaming(mel_output)\n",
        "\n",
        "        expected_length = mel_output.size(1) * 200  # hop_length\n",
        "        print(f\"   ‚úÖ Audio synthesis successful\")\n",
        "        print(f\"   Audio output shape: {audio_output.shape}\")\n",
        "        print(f\"   Expected length: ~{expected_length}\")\n",
        "        print(f\"   Actual length: {audio_output.size(-1)}\")\n",
        "        print(f\"   Audio stats: min={audio_output.min():.3f}, max={audio_output.max():.3f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Audio synthesis failed: {e}\")\n",
        "\n",
        "    # Test complete inference\n",
        "    print(\"\\nüéØ Testing complete inference pipeline...\")\n",
        "    try:\n",
        "        results = model.inference(test_text_ids, return_intermediate=True)\n",
        "\n",
        "        print(f\"   ‚úÖ Complete inference successful\")\n",
        "        print(f\"   Results keys: {list(results.keys())}\")\n",
        "        print(f\"   Audio shape: {results['audio'].shape}\")\n",
        "        print(f\"   Mel shape: {results['mel_spectrogram'].shape}\")\n",
        "        print(f\"   Text length: {results['text_length']}\")\n",
        "        print(f\"   Mel length: {results['mel_length']}\")\n",
        "        print(f\"   Audio length: {results['audio_length']}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Complete inference failed: {e}\")\n",
        "\n",
        "    # Model statistics\n",
        "    print(\"\\nüìä Model Statistics:\")\n",
        "    stats = model.get_model_stats()\n",
        "    for key, value in stats.items():\n",
        "        if key == 'component_parameters':\n",
        "            print(f\"   Component parameters:\")\n",
        "            for comp, params in value.items():\n",
        "                print(f\"     {comp}: {params:,}\")\n",
        "        else:\n",
        "            if isinstance(value, float):\n",
        "                print(f\"   {key}: {value:.2f}\")\n",
        "            else:\n",
        "                print(f\"   {key}: {value:,}\")\n",
        "\n",
        "def validate_model_architecture():\n",
        "    \"\"\"Validate the model architecture meets requirements\"\"\"\n",
        "    print(\"\\n‚úÖ Model Architecture Validation\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    requirements = {\n",
        "        'streaming_capability': False,\n",
        "        'causal_attention': False,\n",
        "        'duration_prediction': False,\n",
        "        'mel_generation': False,\n",
        "        'audio_synthesis': False,\n",
        "        'context_management': False\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Create minimal model for testing\n",
        "        model = StreamingTTSModel(vocab_size=100, hidden_dim=64, num_layers=2)\n",
        "\n",
        "        # Test streaming capability\n",
        "        if hasattr(model, 'reset_streaming_state') and hasattr(model, 'forward_streaming'):\n",
        "            requirements['streaming_capability'] = True\n",
        "\n",
        "        # Test causal attention\n",
        "        if hasattr(model.encoder.layers[0], 'self_attention'):\n",
        "            requirements['causal_attention'] = True\n",
        "\n",
        "        # Test duration prediction\n",
        "        if hasattr(model, 'duration_predictor'):\n",
        "            requirements['duration_prediction'] = True\n",
        "\n",
        "        # Test mel generation\n",
        "        if hasattr(model, 'mel_decoder'):\n",
        "            requirements['mel_generation'] = True\n",
        "\n",
        "        # Test audio synthesis\n",
        "        if hasattr(model, 'vocoder') and hasattr(model, 'synthesize_streaming'):\n",
        "            requirements['audio_synthesis'] = True\n",
        "\n",
        "        # Test context management\n",
        "        if hasattr(model, 'streaming_context'):\n",
        "            requirements['context_management'] = True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Architecture validation failed: {e}\")\n",
        "\n",
        "    print(\"Architecture Requirements:\")\n",
        "    for req, status in requirements.items():\n",
        "        status_icon = \"‚úÖ\" if status else \"‚ùå\"\n",
        "        print(f\"   {status_icon} {req.replace('_', ' ').title()}\")\n",
        "\n",
        "    all_passed = all(requirements.values())\n",
        "    if all_passed:\n",
        "        print(f\"\\nüéâ All architecture requirements met!\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è Some requirements not met - check implementation\")\n",
        "\n",
        "    return all_passed\n",
        "\n",
        "# ==============================================================================\n",
        "# PART 2 SUMMARY AND NEXT STEPS\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ PART 2 COMPLETION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Run component tests\n",
        "test_model_components()\n",
        "\n",
        "# Run complete model test\n",
        "test_complete_model()\n",
        "\n",
        "# Validate architecture\n",
        "architecture_valid = validate_model_architecture()\n",
        "\n",
        "print(f\"\\nüéØ PART 2 COMPLETE - Neural Architecture Ready!\")\n",
        "print(f\"Part 2 delivered:\")\n",
        "print(f\"   ‚úÖ Streaming buffer with overlap management\")\n",
        "print(f\"   ‚úÖ Causal convolutions for streaming\")\n",
        "print(f\"   ‚úÖ Multi-head causal attention with relative positioning\")\n",
        "print(f\"   ‚úÖ Streaming transformer encoder with context\")\n",
        "print(f\"   ‚úÖ Advanced duration predictor\")\n",
        "print(f\"   ‚úÖ Streaming mel-spectrogram decoder\")\n",
        "print(f\"   ‚úÖ High-quality vocoder for LibriSpeech\")\n",
        "print(f\"   ‚úÖ Complete integrated streaming TTS model\")\n",
        "print(f\"   ‚úÖ Comprehensive testing and validation\")\n",
        "\n",
        "print(f\"\\nModel Capabilities:\")\n",
        "print(f\"   üîÑ Real-time word-by-word processing\")\n",
        "print(f\"   üß† Context-aware streaming inference\")\n",
        "print(f\"   ‚è±Ô∏è  Duration-based alignment\")\n",
        "print(f\"   üéº High-quality mel-spectrogram generation\")\n",
        "print(f\"   üîä 16kHz audio synthesis optimized for LibriSpeech\")\n",
        "\n",
        "if architecture_valid:\n",
        "    print(f\"\\n‚úÖ Architecture validation: PASSED\")\n",
        "    print(f\"üöÄ Ready for Part 3: Training Pipeline & Optimization\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è Architecture validation: ISSUES FOUND\")\n",
        "    print(f\"   Please resolve issues before proceeding to Part 3\")\n",
        "\n",
        "print(f\"\\nNext: Part 3 will implement:\")\n",
        "print(f\"   - Advanced training pipeline with mixed precision\")\n",
        "print(f\"   - Loss functions and optimization strategies\")\n",
        "print(f\"   - Validation and checkpointing\")\n",
        "print(f\"   - Performance monitoring and debugging tools\")\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "ePkZPZMDirtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bixeSEDeitlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K452IGVeuuud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MvUt7DBBuuxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ffCKFYjuu0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uQMaoIb3uu3c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}