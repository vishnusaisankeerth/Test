import torch
import torch.nn as nn
import torch.nn.functional as F
import torchaudio
import numpy as np
import librosa
from torch.utils.data import Dataset, DataLoader
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC
import threading
import queue
import time
from collections import deque
import scipy.signal
from IPython.display import Audio, display
import io
from torch.cuda.amp import autocast, GradScaler
import warnings
warnings.filterwarnings(‘ignore’)

class AdvancedFeatureExtractor(nn.Module):
def **init**(self, n_mels=80, n_fft=512, hop_length=160, win_length=400):
super().**init**()
self.n_mels = n_mels
self.n_fft = n_fft
self.hop_length = hop_length
self.win_length = win_length

```
    self.mel_transform = torchaudio.transforms.MelSpectrogram(
        sample_rate=16000,
        n_fft=n_fft,
        hop_length=hop_length,
        win_length=win_length,
        n_mels=n_mels,
        power=2.0
    )
    
    self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()
    
def forward(self, waveform):
    mel_spec = self.mel_transform(waveform)
    log_mel = self.amplitude_to_db(mel_spec)
    return log_mel
```

class ConformerBlock(nn.Module):
def **init**(self, d_model, n_heads, kernel_size=31, dropout=0.1):
super().**init**()
self.ffn1 = nn.Sequential(
nn.LayerNorm(d_model),
nn.Linear(d_model, d_model * 4),
nn.SiLU(),
nn.Dropout(dropout),
nn.Linear(d_model * 4, d_model),
nn.Dropout(dropout)
)

```
    self.mhsa = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)
    self.mhsa_norm = nn.LayerNorm(d_model)
    
    self.conv_norm = nn.LayerNorm(d_model)
    self.conv_module = nn.Sequential(
        nn.Conv1d(d_model, d_model * 2, 1),
        nn.GLU(dim=1),
        nn.Conv1d(d_model, d_model, kernel_size, padding=kernel_size//2, groups=d_model),
        nn.BatchNorm1d(d_model),
        nn.SiLU(),
        nn.Conv1d(d_model, d_model, 1),
        nn.Dropout(dropout)
    )
    
    self.ffn2 = nn.Sequential(
        nn.LayerNorm(d_model),
        nn.Linear(d_model, d_model * 4),
        nn.SiLU(),
        nn.Dropout(dropout),
        nn.Linear(d_model * 4, d_model),
        nn.Dropout(dropout)
    )
    
    self.final_norm = nn.LayerNorm(d_model)
    
def forward(self, x):
    x = x + 0.5 * self.ffn1(x)
    
    attn_out, _ = self.mhsa(self.mhsa_norm(x), self.mhsa_norm(x), self.mhsa_norm(x))
    x = x + attn_out
    
    # Apply LayerNorm before conv operations
    conv_input = self.conv_norm(x)
    conv_input = conv_input.transpose(1, 2)  # [B, T, D] -> [B, D, T]
    conv_out = self.conv_module(conv_input)
    conv_out = conv_out.transpose(1, 2)  # [B, D, T] -> [B, T, D]
    x = x + conv_out
    
    x = x + 0.5 * self.ffn2(x)
    return self.final_norm(x)
```

class StreamingConformerASR(nn.Module):
def **init**(self, vocab_size=28, d_model=512, n_layers=12, n_heads=8, kernel_size=31):
super().**init**()
self.feature_extractor = AdvancedFeatureExtractor()

```
    self.input_projection = nn.Linear(80, d_model)
    self.pos_encoding = nn.Parameter(torch.randn(1, 5000, d_model) * 0.02)
    
    self.conformer_blocks = nn.ModuleList([
        ConformerBlock(d_model, n_heads, kernel_size) for _ in range(n_layers)
    ])
    
    self.output_projection = nn.Linear(d_model, vocab_size)
    self.dropout = nn.Dropout(0.1)
    
def forward(self, waveform, lengths=None):
    features = self.feature_extractor(waveform)
    B, n_mels, T = features.shape
    features = features.transpose(1, 2)
    
    x = self.input_projection(features)
    seq_len = x.size(1)
    x = x + self.pos_encoding[:, :seq_len, :]
    x = self.dropout(x)
    
    for block in self.conformer_blocks:
        x = block(x)
        
    logits = self.output_projection(x)
    return F.log_softmax(logits, dim=-1)
```

class CharTokenizer:
def **init**(self):
self.chars = “ ABCDEFGHIJKLMNOPQRSTUVWXYZ’”
self.char_to_idx = {char: idx for idx, char in enumerate(self.chars)}
self.idx_to_char = {idx: char for idx, char in enumerate(self.chars)}
self.blank_idx = 0
self.vocab_size = len(self.chars)

```
def encode(self, text):
    text = text.upper()
    return [self.char_to_idx.get(char, self.char_to_idx[' ']) for char in text]

def decode(self, indices):
    chars = [self.idx_to_char.get(idx, '') for idx in indices if idx != self.blank_idx]
    return ''.join(chars).strip()
```

class LibriSpeechDataset(Dataset):
def **init**(self, root_path=’/content/drive/MyDrive/NAC’, split=‘train-clean-100’, max_length=16000*10):
if split == ‘train-clean-100’ and root_path == ‘/content/drive/MyDrive/NAC’:
self.data_path = ‘/content/drive/MyDrive/NAC/train-clean-100’
self.use_local = True
else:
self.data_path = root_path
self.use_local = False

```
    if self.use_local:
        self.audio_files = []
        self.transcripts = []
        self._load_local_data()
    else:
        self.dataset = torchaudio.datasets.LIBRISPEECH(root_path, url=split, download=True)
        
    self.max_length = max_length
    self.tokenizer = CharTokenizer()

def _load_local_data(self):
    import os
    import glob
    
    transcript_files = glob.glob(os.path.join(self.data_path, '**', '*.trans.txt'), recursive=True)
    
    for transcript_file in transcript_files:
        with open(transcript_file, 'r') as f:
            for line in f:
                parts = line.strip().split(' ', 1)
                if len(parts) == 2:
                    file_id, transcript = parts
                    audio_file = os.path.join(os.path.dirname(transcript_file), file_id + '.flac')
                    if os.path.exists(audio_file):
                        self.audio_files.append(audio_file)
                        self.transcripts.append(transcript)
    
    print(f"Loaded {len(self.audio_files)} audio files from local dataset")
    
def __len__(self):
    if self.use_local:
        return len(self.audio_files)
    else:
        return len(self.dataset)

def __getitem__(self, idx):
    if self.use_local:
        audio_file = self.audio_files[idx]
        transcript = self.transcripts[idx]
        
        waveform, sample_rate = torchaudio.load(audio_file)
        
        if sample_rate != 16000:
            resampler = torchaudio.transforms.Resample(sample_rate, 16000)
            waveform = resampler(waveform)
        
        waveform = waveform.squeeze(0)
        if len(waveform) > self.max_length:
            waveform = waveform[:self.max_length]
        
        labels = self.tokenizer.encode(transcript)
        
        return waveform, torch.tensor(labels, dtype=torch.long), transcript
    else:
        waveform, sample_rate, transcript, speaker_id, chapter_id, utterance_id = self.dataset[idx]
        
        if sample_rate != 16000:
            resampler = torchaudio.transforms.Resample(sample_rate, 16000)
            waveform = resampler(waveform)
        
        waveform = waveform.squeeze(0)
        if len(waveform) > self.max_length:
            waveform = waveform[:self.max_length]
        
        labels = self.tokenizer.encode(transcript)
            
        return waveform, torch.tensor(labels, dtype=torch.long), transcript
```

class CTCLoss(nn.Module):
def **init**(self, blank_idx=0):
super().**init**()
self.ctc_loss = nn.CTCLoss(blank=blank_idx, reduction=‘mean’, zero_infinity=True)

```
def forward(self, log_probs, targets, input_lengths, target_lengths):
    log_probs = log_probs.transpose(0, 1)
    return self.ctc_loss(log_probs, targets, input_lengths, target_lengths)
```

class RealTimeASRTrainer:
def **init**(self, model, device=‘cuda’):
self.model = model.to(device)
self.device = device
self.criterion = CTCLoss()
self.optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-6)
self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, T_0=1000)
self.scaler = GradScaler()

```
def train_epoch(self, dataloader, epoch):
    self.model.train()
    total_loss = 0
    
    for batch_idx, (waveforms, labels, transcripts) in enumerate(dataloader):
        # Move to GPU with non_blocking for efficiency
        waveforms = waveforms.to(self.device, non_blocking=True)
        labels = labels.to(self.device, non_blocking=True)
        
        # Calculate lengths more efficiently
        input_lengths = torch.full((waveforms.size(0),), waveforms.size(-1) // 160, dtype=torch.long, device=self.device)
        target_lengths = torch.tensor([torch.count_nonzero(label).item() for label in labels], 
                                    dtype=torch.long, device=self.device)
        
        # Flatten labels for CTC
        labels_flat = []
        for label in labels:
            valid_tokens = label[label != 0]
            if len(valid_tokens) > 0:
                labels_flat.extend(valid_tokens.tolist())
            else:
                labels_flat.append(0)  # Add blank if no valid tokens
        
        labels_flat = torch.tensor(labels_flat, dtype=torch.long, device=self.device)
        
        self.optimizer.zero_grad()
        
        with autocast():
            log_probs = self.model(waveforms)
            # Ensure we have valid target lengths
            target_lengths = torch.clamp(target_lengths, min=1)
            loss = self.criterion(log_probs, labels_flat, input_lengths, target_lengths)
        
        if torch.isfinite(loss):
            self.scaler.scale(loss).backward()
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.scaler.step(self.optimizer)
            self.scaler.update()
            self.scheduler.step()
            
            total_loss += loss.item()
        
        if batch_idx % 50 == 0:
            print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')
            
    return total_loss / len(dataloader)
```

class RealTimeStreamer:
def **init**(self, model, device=‘cuda’, chunk_duration=0.32, overlap=0.16):
self.model = model.to(device)
self.model.eval()
self.device = device
self.chunk_duration = chunk_duration
self.overlap = overlap
self.sample_rate = 16000
self.chunk_size = int(chunk_duration * self.sample_rate)
self.overlap_size = int(overlap * self.sample_rate)

```
    self.tokenizer = CharTokenizer()
    
    self.audio_buffer = deque(maxlen=self.chunk_size * 3)
    self.text_buffer = deque(maxlen=10)
    
    self.audio_queue = queue.Queue()
    self.result_queue = queue.Queue()
    
    self.is_streaming = False
    
def simulate_streaming_from_file(self, audio_file_path):
    waveform, sr = torchaudio.load(audio_file_path)
    if sr != self.sample_rate:
        resampler = torchaudio.transforms.Resample(sr, self.sample_rate)
        waveform = resampler(waveform)
    
    waveform = waveform.squeeze(0).numpy()
    
    self.is_streaming = True
    processing_thread = threading.Thread(target=self._simulate_realtime_processing, args=(waveform,))
    processing_thread.start()
    
    return processing_thread

def _simulate_realtime_processing(self, waveform):
    step_size = self.chunk_size - self.overlap_size
    
    for i in range(0, len(waveform) - self.chunk_size, step_size):
        if not self.is_streaming:
            break
            
        chunk = waveform[i:i + self.chunk_size]
        
        is_speech = self._detect_speech_energy(chunk)
        if is_speech:
            transcription = self.process_audio_chunk(chunk)
            if transcription.strip():
                self.result_queue.put(transcription)
        
        time.sleep(self.chunk_duration * 0.8)

def _detect_speech_energy(self, audio_chunk):
    energy = np.sum(audio_chunk ** 2) / len(audio_chunk)
    return energy > 0.001

def process_from_microphone_data(self, audio_data):
    self.is_streaming = True
    
    for i in range(0, len(audio_data) - self.chunk_size, self.chunk_size // 2):
        chunk = audio_data[i:i + self.chunk_size]
        
        is_speech = self._detect_speech_energy(chunk)
        if is_speech:
            transcription = self.process_audio_chunk(chunk)
            if transcription.strip():
                yield transcription

def process_audio_chunk(self, audio_chunk):
    if len(audio_chunk) < self.chunk_size:
        return ""
        
    audio_chunk = torch.tensor(audio_chunk, dtype=torch.float32).unsqueeze(0).to(self.device)
    
    with torch.no_grad():
        with autocast():
            log_probs = self.model(audio_chunk)
            predicted_ids = torch.argmax(log_probs, dim=-1)
            
    transcription = self.tokenizer.decode(predicted_ids[0].cpu().numpy())
    return transcription

def start_streaming_from_file(self, audio_file):
    print(f"Processing audio file: {audio_file}")
    thread = self.simulate_streaming_from_file(audio_file)
    return thread

def _process_audio_loop_from_data(self, audio_data):
    step_size = self.chunk_size // 2
    
    for i in range(0, len(audio_data) - self.chunk_size, step_size):
        if not self.is_streaming:
            break
            
        chunk = audio_data[i:i + self.chunk_size]
        
        is_speech = self._detect_speech_energy(chunk)
        if is_speech:
            transcription = self.process_audio_chunk(chunk)
            if transcription.strip():
                self.result_queue.put(transcription)

def process_uploaded_audio(self, audio_file_path):
    results = []
    
    waveform, sr = torchaudio.load(audio_file_path)
    if sr != self.sample_rate:
        resampler = torchaudio.transforms.Resample(sr, self.sample_rate)
        waveform = resampler(waveform)
    
    waveform = waveform.squeeze(0).numpy()
    
    print(f"Processing {len(waveform)/self.sample_rate:.2f} seconds of audio...")
    
    step_size = self.chunk_size // 2
    for i in range(0, len(waveform) - self.chunk_size, step_size):
        chunk = waveform[i:i + self.chunk_size]
        
        if self._detect_speech_energy(chunk):
            transcription = self.process_audio_chunk(chunk)
            if transcription.strip():
                timestamp = i / self.sample_rate
                results.append((timestamp, transcription))
                print(f"[{timestamp:.2f}s] {transcription}")
    
    return results

def get_latest_transcription(self):
    try:
        return self.result_queue.get_nowait()
    except queue.Empty:
        return None

def stop_streaming(self):
    self.is_streaming = False
```

def collate_fn(batch):
waveforms, labels, transcripts = zip(*batch)

```
max_waveform_len = max(w.size(0) for w in waveforms)
padded_waveforms = torch.zeros(len(waveforms), max_waveform_len)

for i, w in enumerate(waveforms):
    padded_waveforms[i, :w.size(0)] = w

max_label_len = max(len(label) for label in labels)
padded_labels = torch.zeros(len(labels), max_label_len, dtype=torch.long)

for i, label in enumerate(labels):
    padded_labels[i, :len(label)] = label

return padded_waveforms, padded_labels, transcripts
```

def train_model():
device = torch.device(‘cuda’ if torch.cuda.is_available() else ‘cpu’)
print(f”Training on device: {device}”)

```
# Enable GPU optimizations
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False

tokenizer = CharTokenizer()
model = StreamingConformerASR(vocab_size=tokenizer.vocab_size, d_model=512, n_layers=12)
trainer = RealTimeASRTrainer(model, device)

dataset = LibriSpeechDataset('/content/drive/MyDrive/NAC', split='train-clean-100')
dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn, 
                      num_workers=2, pin_memory=True, persistent_workers=True)

print("Starting training...")
for epoch in range(10):
    avg_loss = trainer.train_epoch(dataloader, epoch)
    print(f"Epoch {epoch}: Average Loss = {avg_loss:.4f}")
    
    if epoch % 2 == 0:
        torch.save(model.state_dict(), f'conformer_asr_epoch_{epoch}.pth')

torch.save(model.state_dict(), 'conformer_asr_final.pth')
return model
```

def load_pretrained_model(checkpoint_path=None):
device = torch.device(‘cuda’ if torch.cuda.is_available() else ‘cpu’)
tokenizer = CharTokenizer()
model = StreamingConformerASR(vocab_size=tokenizer.vocab_size, d_model=512, n_layers=12)

```
if checkpoint_path and torch.cuda.is_available():
    model.load_state_dict(torch.load(checkpoint_path))
    print(f"Loaded model from {checkpoint_path}")
else:
    print("Using randomly initialized model")

return model
```

def demo_file_transcription():
model = load_pretrained_model()
streamer = RealTimeStreamer(model)

```
print("Real-time Speech-to-Text Demo")
print("Using local LibriSpeech data")

dataset = LibriSpeechDataset('/content/drive/MyDrive/NAC', split='train-clean-100')
sample_audio, sample_labels, transcript = dataset[0]

temp_audio_path = "temp_sample.wav"
torchaudio.save(temp_audio_path, sample_audio.unsqueeze(0), 16000)

print(f"Original transcript: {transcript}")
print("Processing with real-time system...")

results = streamer.process_uploaded_audio(temp_audio_path)

full_transcription = " ".join([result[1] for result in results])
print(f"\nFull transcription: {full_transcription}")

return results
```

def demo_realtime_simulation():
model = load_pretrained_model()
streamer = RealTimeStreamer(model)

```
dataset = LibriSpeechDataset('/content/drive/MyDrive/NAC', split='train-clean-100')
sample_audio, sample_labels, transcript = dataset[0]

temp_audio_path = "temp_sample.wav"
torchaudio.save(temp_audio_path, sample_audio.unsqueeze(0), 16000)

print("Simulating real-time transcription...")
print(f"Original: {transcript}")
print("Real-time output:")

thread = streamer.start_streaming_from_file(temp_audio_path)

try:
    while thread.is_alive():
        transcription = streamer.get_latest_transcription()
        if transcription:
            print(f">> {transcription}")
        time.sleep(0.1)
except KeyboardInterrupt:
    streamer.stop_streaming()

thread.join()
```

def process_your_audio_file(file_path):
model = load_pretrained_model()
streamer = RealTimeStreamer(model)

```
print(f"Processing your audio file: {file_path}")
results = streamer.process_uploaded_audio(file_path)

print("\n=== FINAL TRANSCRIPTION ===")
full_text = " ".join([result[1] for result in results])
print(full_text)

return full_text
```

if **name** == “**main**”:
print(“Ultra-Low Latency Real-Time Speech-to-Text System”)
print(“1. Train new model”)
print(“2. Demo with sample audio file”)
print(“3. Process your own audio file”)
print(“4. Simulate real-time processing”)

```
choice = input("Enter choice (1-4): ")

if choice == "1":
    model = train_model()
    print("Training completed!")
elif choice == "2":
    demo_file_transcription()
elif choice == "3":
    file_path = input("Enter audio file path: ")
    process_your_audio_file(file_path)
elif choice == "4":
    demo_realtime_simulation()
else:
    print("Invalid choice")
```

# Jupyter notebook functions

def quick_train():
return train_model()

def quick_demo():
return demo_file_transcription()

def transcribe_file(file_path):
return process_your_audio_file(file_path)
