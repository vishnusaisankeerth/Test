{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V28","mount_file_id":"1tFsumRh0Sz1c0tTmSFNApri8oO9yjeMw","authorship_tag":"ABX9TyNFD7o9+7QLr7P9Ac5N5OK/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e5ZbemIETitm","outputId":"91586aeb-8ca5-45ec-d44b-d47fc712d23d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Ultra-Low Latency Real-Time Speech-to-Text System\n","1. Train new model\n","2. Demo with sample audio file\n","3. Process your own audio file\n","4. Simulate real-time processing\n","Enter choice (1-4): 1\n","Training on device: cpu\n","Loaded 28539 audio files from local dataset\n","Starting training...\n","Epoch 0, Batch 0, Loss: 13.9864\n","Epoch 0, Batch 100, Loss: 2.9739\n","Epoch 0, Batch 200, Loss: 3.0648\n","Epoch 0, Batch 300, Loss: 2.9117\n","Epoch 0, Batch 400, Loss: 2.9489\n","Epoch 0, Batch 500, Loss: 2.8882\n","Epoch 0, Batch 600, Loss: 2.8909\n","Epoch 0, Batch 700, Loss: 2.8559\n","Epoch 0, Batch 800, Loss: 2.8752\n","Epoch 0, Batch 900, Loss: 2.8505\n","Epoch 0, Batch 1000, Loss: 2.8396\n","Epoch 0, Batch 1100, Loss: 2.9615\n","Epoch 0, Batch 1200, Loss: 3.0167\n","Epoch 0, Batch 1300, Loss: 2.9505\n","Epoch 0, Batch 1400, Loss: 2.8733\n","Epoch 0, Batch 1500, Loss: 2.7683\n","Epoch 0, Batch 1600, Loss: 2.8713\n","Epoch 0, Batch 1700, Loss: 2.9265\n","Epoch 0, Batch 1800, Loss: 2.8288\n","Epoch 0, Batch 1900, Loss: 2.8372\n","Epoch 0, Batch 2000, Loss: 2.8808\n","Epoch 0, Batch 2100, Loss: 2.9203\n","Epoch 0, Batch 2200, Loss: 2.9799\n","Epoch 0, Batch 2300, Loss: 2.7648\n","Epoch 0, Batch 2400, Loss: 2.8458\n","Epoch 0, Batch 2500, Loss: 2.9419\n","Epoch 0, Batch 2600, Loss: 2.8181\n","Epoch 0, Batch 2700, Loss: 2.7422\n","Epoch 0, Batch 2800, Loss: 2.7506\n","Epoch 0, Batch 2900, Loss: 2.7271\n","Epoch 0, Batch 3000, Loss: 2.7965\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchaudio\n","import numpy as np\n","import librosa\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n","import threading\n","import queue\n","import time\n","from collections import deque\n","import scipy.signal\n","from IPython.display import Audio, display\n","import io\n","from torch.cuda.amp import autocast, GradScaler\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","class AdvancedFeatureExtractor(nn.Module):\n","    def __init__(self, n_mels=80, n_fft=512, hop_length=160, win_length=400):\n","        super().__init__()\n","        self.n_mels = n_mels\n","        self.n_fft = n_fft\n","        self.hop_length = hop_length\n","        self.win_length = win_length\n","\n","        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n","            sample_rate=16000,\n","            n_fft=n_fft,\n","            hop_length=hop_length,\n","            win_length=win_length,\n","            n_mels=n_mels,\n","            power=2.0\n","        )\n","\n","        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()\n","\n","    def forward(self, waveform):\n","        mel_spec = self.mel_transform(waveform)\n","        log_mel = self.amplitude_to_db(mel_spec)\n","        return log_mel\n","\n","class ConformerBlock(nn.Module):\n","    def __init__(self, d_model, n_heads, kernel_size=31, dropout=0.1):\n","        super().__init__()\n","        self.ffn1 = nn.Sequential(\n","            nn.LayerNorm(d_model),\n","            nn.Linear(d_model, d_model * 4),\n","            nn.SiLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(d_model * 4, d_model),\n","            nn.Dropout(dropout)\n","        )\n","\n","        self.mhsa = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n","        self.mhsa_norm = nn.LayerNorm(d_model)\n","\n","        self.conv_norm = nn.LayerNorm(d_model)\n","        self.conv_module = nn.Sequential(\n","            nn.Conv1d(d_model, d_model * 2, 1),\n","            nn.GLU(dim=1),\n","            nn.Conv1d(d_model, d_model, kernel_size, padding=kernel_size//2, groups=d_model),\n","            nn.BatchNorm1d(d_model),\n","            nn.SiLU(),\n","            nn.Conv1d(d_model, d_model, 1),\n","            nn.Dropout(dropout)\n","        )\n","\n","        self.ffn2 = nn.Sequential(\n","            nn.LayerNorm(d_model),\n","            nn.Linear(d_model, d_model * 4),\n","            nn.SiLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(d_model * 4, d_model),\n","            nn.Dropout(dropout)\n","        )\n","\n","        self.final_norm = nn.LayerNorm(d_model)\n","\n","    def forward(self, x):\n","        x = x + 0.5 * self.ffn1(x)\n","\n","        attn_out, _ = self.mhsa(self.mhsa_norm(x), self.mhsa_norm(x), self.mhsa_norm(x))\n","        x = x + attn_out\n","\n","        # Apply LayerNorm before conv operations\n","        conv_input = self.conv_norm(x)\n","        conv_input = conv_input.transpose(1, 2)  # [B, T, D] -> [B, D, T]\n","        conv_out = self.conv_module(conv_input)\n","        conv_out = conv_out.transpose(1, 2)  # [B, D, T] -> [B, T, D]\n","        x = x + conv_out\n","\n","        x = x + 0.5 * self.ffn2(x)\n","        return self.final_norm(x)\n","\n","class StreamingConformerASR(nn.Module):\n","    def __init__(self, vocab_size=28, d_model=512, n_layers=12, n_heads=8, kernel_size=31):\n","        super().__init__()\n","        self.feature_extractor = AdvancedFeatureExtractor()\n","\n","        self.input_projection = nn.Linear(80, d_model)\n","        self.pos_encoding = nn.Parameter(torch.randn(1, 5000, d_model) * 0.02)\n","\n","        self.conformer_blocks = nn.ModuleList([\n","            ConformerBlock(d_model, n_heads, kernel_size) for _ in range(n_layers)\n","        ])\n","\n","        self.output_projection = nn.Linear(d_model, vocab_size)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, waveform, lengths=None):\n","        features = self.feature_extractor(waveform)\n","        B, n_mels, T = features.shape\n","        features = features.transpose(1, 2)\n","\n","        x = self.input_projection(features)\n","        seq_len = x.size(1)\n","        x = x + self.pos_encoding[:, :seq_len, :]\n","        x = self.dropout(x)\n","\n","        for block in self.conformer_blocks:\n","            x = block(x)\n","\n","        logits = self.output_projection(x)\n","        return F.log_softmax(logits, dim=-1)\n","\n","class CharTokenizer:\n","    def __init__(self):\n","        self.chars = \" ABCDEFGHIJKLMNOPQRSTUVWXYZ'\"\n","        self.char_to_idx = {char: idx for idx, char in enumerate(self.chars)}\n","        self.idx_to_char = {idx: char for idx, char in enumerate(self.chars)}\n","        self.blank_idx = 0\n","        self.vocab_size = len(self.chars)\n","\n","    def encode(self, text):\n","        text = text.upper()\n","        return [self.char_to_idx.get(char, self.char_to_idx[' ']) for char in text]\n","\n","    def decode(self, indices):\n","        chars = [self.idx_to_char.get(idx, '') for idx in indices if idx != self.blank_idx]\n","        return ''.join(chars).strip()\n","\n","class LibriSpeechDataset(Dataset):\n","    def __init__(self, root_path='/content/drive/MyDrive/NAC', split='train-clean-100', max_length=16000*10):\n","        if split == 'train-clean-100' and root_path == '/content/drive/MyDrive/NAC':\n","            self.data_path = '/content/drive/MyDrive/NAC/train-clean-100'\n","            self.use_local = True\n","        else:\n","            self.data_path = root_path\n","            self.use_local = False\n","\n","        if self.use_local:\n","            self.audio_files = []\n","            self.transcripts = []\n","            self._load_local_data()\n","        else:\n","            self.dataset = torchaudio.datasets.LIBRISPEECH(root_path, url=split, download=True)\n","\n","        self.max_length = max_length\n","        self.tokenizer = CharTokenizer()\n","\n","    def _load_local_data(self):\n","        import os\n","        import glob\n","\n","        transcript_files = glob.glob(os.path.join(self.data_path, '**', '*.trans.txt'), recursive=True)\n","\n","        for transcript_file in transcript_files:\n","            with open(transcript_file, 'r') as f:\n","                for line in f:\n","                    parts = line.strip().split(' ', 1)\n","                    if len(parts) == 2:\n","                        file_id, transcript = parts\n","                        audio_file = os.path.join(os.path.dirname(transcript_file), file_id + '.flac')\n","                        if os.path.exists(audio_file):\n","                            self.audio_files.append(audio_file)\n","                            self.transcripts.append(transcript)\n","\n","        print(f\"Loaded {len(self.audio_files)} audio files from local dataset\")\n","\n","    def __len__(self):\n","        if self.use_local:\n","            return len(self.audio_files)\n","        else:\n","            return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        if self.use_local:\n","            audio_file = self.audio_files[idx]\n","            transcript = self.transcripts[idx]\n","\n","            waveform, sample_rate = torchaudio.load(audio_file)\n","\n","            if sample_rate != 16000:\n","                resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n","                waveform = resampler(waveform)\n","\n","            waveform = waveform.squeeze(0)\n","            if len(waveform) > self.max_length:\n","                waveform = waveform[:self.max_length]\n","\n","            labels = self.tokenizer.encode(transcript)\n","\n","            return waveform, torch.tensor(labels, dtype=torch.long), transcript\n","        else:\n","            waveform, sample_rate, transcript, speaker_id, chapter_id, utterance_id = self.dataset[idx]\n","\n","            if sample_rate != 16000:\n","                resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n","                waveform = resampler(waveform)\n","\n","            waveform = waveform.squeeze(0)\n","            if len(waveform) > self.max_length:\n","                waveform = waveform[:self.max_length]\n","\n","            labels = self.tokenizer.encode(transcript)\n","\n","            return waveform, torch.tensor(labels, dtype=torch.long), transcript\n","\n","class CTCLoss(nn.Module):\n","    def __init__(self, blank_idx=0):\n","        super().__init__()\n","        self.ctc_loss = nn.CTCLoss(blank=blank_idx, reduction='mean', zero_infinity=True)\n","\n","    def forward(self, log_probs, targets, input_lengths, target_lengths):\n","        log_probs = log_probs.transpose(0, 1)\n","        return self.ctc_loss(log_probs, targets, input_lengths, target_lengths)\n","\n","class RealTimeASRTrainer:\n","    def __init__(self, model, device='cuda'):\n","        self.model = model.to(device)\n","        self.device = device\n","        self.criterion = CTCLoss()\n","        self.optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-6)\n","        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, T_0=1000)\n","        self.scaler = GradScaler()\n","\n","    def train_epoch(self, dataloader, epoch):\n","        self.model.train()\n","        total_loss = 0\n","\n","        for batch_idx, (waveforms, labels, transcripts) in enumerate(dataloader):\n","            waveforms = waveforms.to(self.device)\n","            labels = labels.to(self.device)\n","\n","            input_lengths = torch.full((waveforms.size(0),), waveforms.size(-1) // 160, dtype=torch.long)\n","            target_lengths = torch.tensor([torch.count_nonzero(label).item() for label in labels], dtype=torch.long)\n","\n","            input_lengths = input_lengths.to(self.device)\n","            target_lengths = target_lengths.to(self.device)\n","\n","            labels_flat = []\n","            for label in labels:\n","                labels_flat.extend(label[label != 0].tolist())\n","            labels_flat = torch.tensor(labels_flat, dtype=torch.long).to(self.device)\n","\n","            self.optimizer.zero_grad()\n","\n","            with autocast():\n","                log_probs = self.model(waveforms)\n","                loss = self.criterion(log_probs, labels_flat, input_lengths, target_lengths)\n","\n","            self.scaler.scale(loss).backward()\n","            self.scaler.unscale_(self.optimizer)\n","            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n","            self.scaler.step(self.optimizer)\n","            self.scaler.update()\n","            self.scheduler.step()\n","\n","            total_loss += loss.item()\n","\n","            if batch_idx % 100 == 0:\n","                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n","\n","        return total_loss / len(dataloader)\n","\n","class RealTimeStreamer:\n","    def __init__(self, model, device='cuda', chunk_duration=0.32, overlap=0.16):\n","        self.model = model.to(device)\n","        self.model.eval()\n","        self.device = device\n","        self.chunk_duration = chunk_duration\n","        self.overlap = overlap\n","        self.sample_rate = 16000\n","        self.chunk_size = int(chunk_duration * self.sample_rate)\n","        self.overlap_size = int(overlap * self.sample_rate)\n","\n","        self.tokenizer = CharTokenizer()\n","\n","        self.audio_buffer = deque(maxlen=self.chunk_size * 3)\n","        self.text_buffer = deque(maxlen=10)\n","\n","        self.audio_queue = queue.Queue()\n","        self.result_queue = queue.Queue()\n","\n","        self.is_streaming = False\n","\n","    def simulate_streaming_from_file(self, audio_file_path):\n","        waveform, sr = torchaudio.load(audio_file_path)\n","        if sr != self.sample_rate:\n","            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n","            waveform = resampler(waveform)\n","\n","        waveform = waveform.squeeze(0).numpy()\n","\n","        self.is_streaming = True\n","        processing_thread = threading.Thread(target=self._simulate_realtime_processing, args=(waveform,))\n","        processing_thread.start()\n","\n","        return processing_thread\n","\n","    def _simulate_realtime_processing(self, waveform):\n","        step_size = self.chunk_size - self.overlap_size\n","\n","        for i in range(0, len(waveform) - self.chunk_size, step_size):\n","            if not self.is_streaming:\n","                break\n","\n","            chunk = waveform[i:i + self.chunk_size]\n","\n","            is_speech = self._detect_speech_energy(chunk)\n","            if is_speech:\n","                transcription = self.process_audio_chunk(chunk)\n","                if transcription.strip():\n","                    self.result_queue.put(transcription)\n","\n","            time.sleep(self.chunk_duration * 0.8)\n","\n","    def _detect_speech_energy(self, audio_chunk):\n","        energy = np.sum(audio_chunk ** 2) / len(audio_chunk)\n","        return energy > 0.001\n","\n","    def process_from_microphone_data(self, audio_data):\n","        self.is_streaming = True\n","\n","        for i in range(0, len(audio_data) - self.chunk_size, self.chunk_size // 2):\n","            chunk = audio_data[i:i + self.chunk_size]\n","\n","            is_speech = self._detect_speech_energy(chunk)\n","            if is_speech:\n","                transcription = self.process_audio_chunk(chunk)\n","                if transcription.strip():\n","                    yield transcription\n","\n","    def process_audio_chunk(self, audio_chunk):\n","        if len(audio_chunk) < self.chunk_size:\n","            return \"\"\n","\n","        audio_chunk = torch.tensor(audio_chunk, dtype=torch.float32).unsqueeze(0).to(self.device)\n","\n","        with torch.no_grad():\n","            with autocast():\n","                log_probs = self.model(audio_chunk)\n","                predicted_ids = torch.argmax(log_probs, dim=-1)\n","\n","        transcription = self.tokenizer.decode(predicted_ids[0].cpu().numpy())\n","        return transcription\n","\n","    def start_streaming_from_file(self, audio_file):\n","        print(f\"Processing audio file: {audio_file}\")\n","        thread = self.simulate_streaming_from_file(audio_file)\n","        return thread\n","\n","    def _process_audio_loop_from_data(self, audio_data):\n","        step_size = self.chunk_size // 2\n","\n","        for i in range(0, len(audio_data) - self.chunk_size, step_size):\n","            if not self.is_streaming:\n","                break\n","\n","            chunk = audio_data[i:i + self.chunk_size]\n","\n","            is_speech = self._detect_speech_energy(chunk)\n","            if is_speech:\n","                transcription = self.process_audio_chunk(chunk)\n","                if transcription.strip():\n","                    self.result_queue.put(transcription)\n","\n","    def process_uploaded_audio(self, audio_file_path):\n","        results = []\n","\n","        waveform, sr = torchaudio.load(audio_file_path)\n","        if sr != self.sample_rate:\n","            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n","            waveform = resampler(waveform)\n","\n","        waveform = waveform.squeeze(0).numpy()\n","\n","        print(f\"Processing {len(waveform)/self.sample_rate:.2f} seconds of audio...\")\n","\n","        step_size = self.chunk_size // 2\n","        for i in range(0, len(waveform) - self.chunk_size, step_size):\n","            chunk = waveform[i:i + self.chunk_size]\n","\n","            if self._detect_speech_energy(chunk):\n","                transcription = self.process_audio_chunk(chunk)\n","                if transcription.strip():\n","                    timestamp = i / self.sample_rate\n","                    results.append((timestamp, transcription))\n","                    print(f\"[{timestamp:.2f}s] {transcription}\")\n","\n","        return results\n","\n","    def get_latest_transcription(self):\n","        try:\n","            return self.result_queue.get_nowait()\n","        except queue.Empty:\n","            return None\n","\n","    def stop_streaming(self):\n","        self.is_streaming = False\n","\n","def collate_fn(batch):\n","    waveforms, labels, transcripts = zip(*batch)\n","\n","    max_waveform_len = max(w.size(0) for w in waveforms)\n","    padded_waveforms = torch.zeros(len(waveforms), max_waveform_len)\n","\n","    for i, w in enumerate(waveforms):\n","        padded_waveforms[i, :w.size(0)] = w\n","\n","    max_label_len = max(len(label) for label in labels)\n","    padded_labels = torch.zeros(len(labels), max_label_len, dtype=torch.long)\n","\n","    for i, label in enumerate(labels):\n","        padded_labels[i, :len(label)] = label\n","\n","    return padded_waveforms, padded_labels, transcripts\n","\n","def train_model():\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"Training on device: {device}\")\n","\n","    # Enable GPU optimizations\n","    if torch.cuda.is_available():\n","        torch.backends.cudnn.benchmark = True\n","        torch.backends.cudnn.deterministic = False\n","\n","    tokenizer = CharTokenizer()\n","    model = StreamingConformerASR(vocab_size=tokenizer.vocab_size, d_model=512, n_layers=12)\n","    trainer = RealTimeASRTrainer(model, device)\n","\n","    dataset = LibriSpeechDataset('/content/drive/MyDrive/NAC', split='train-clean-100')\n","    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn,\n","                          num_workers=2, pin_memory=True, persistent_workers=True)\n","\n","    print(\"Starting training...\")\n","    for epoch in range(10):\n","        avg_loss = trainer.train_epoch(dataloader, epoch)\n","        print(f\"Epoch {epoch}: Average Loss = {avg_loss:.4f}\")\n","\n","        if epoch % 2 == 0:\n","            torch.save(model.state_dict(), f'conformer_asr_epoch_{epoch}.pth')\n","\n","    torch.save(model.state_dict(), 'conformer_asr_final.pth')\n","    return model\n","\n","def load_pretrained_model(checkpoint_path=None):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    tokenizer = CharTokenizer()\n","    model = StreamingConformerASR(vocab_size=tokenizer.vocab_size, d_model=512, n_layers=12)\n","\n","    if checkpoint_path and torch.cuda.is_available():\n","        model.load_state_dict(torch.load(checkpoint_path))\n","        print(f\"Loaded model from {checkpoint_path}\")\n","    else:\n","        print(\"Using randomly initialized model\")\n","\n","    return model\n","\n","def demo_file_transcription():\n","    model = load_pretrained_model()\n","    streamer = RealTimeStreamer(model)\n","\n","    print(\"Real-time Speech-to-Text Demo\")\n","    print(\"Using local LibriSpeech data\")\n","\n","    dataset = LibriSpeechDataset('/content/drive/MyDrive/NAC', split='train-clean-100')\n","    sample_audio, sample_labels, transcript = dataset[0]\n","\n","    temp_audio_path = \"temp_sample.wav\"\n","    torchaudio.save(temp_audio_path, sample_audio.unsqueeze(0), 16000)\n","\n","    print(f\"Original transcript: {transcript}\")\n","    print(\"Processing with real-time system...\")\n","\n","    results = streamer.process_uploaded_audio(temp_audio_path)\n","\n","    full_transcription = \" \".join([result[1] for result in results])\n","    print(f\"\\nFull transcription: {full_transcription}\")\n","\n","    return results\n","\n","def demo_realtime_simulation():\n","    model = load_pretrained_model()\n","    streamer = RealTimeStreamer(model)\n","\n","    dataset = LibriSpeechDataset('/content/drive/MyDrive/NAC', split='train-clean-100')\n","    sample_audio, sample_labels, transcript = dataset[0]\n","\n","    temp_audio_path = \"temp_sample.wav\"\n","    torchaudio.save(temp_audio_path, sample_audio.unsqueeze(0), 16000)\n","\n","    print(\"Simulating real-time transcription...\")\n","    print(f\"Original: {transcript}\")\n","    print(\"Real-time output:\")\n","\n","    thread = streamer.start_streaming_from_file(temp_audio_path)\n","\n","    try:\n","        while thread.is_alive():\n","            transcription = streamer.get_latest_transcription()\n","            if transcription:\n","                print(f\">> {transcription}\")\n","            time.sleep(0.1)\n","    except KeyboardInterrupt:\n","        streamer.stop_streaming()\n","\n","    thread.join()\n","\n","def process_your_audio_file(file_path):\n","    model = load_pretrained_model()\n","    streamer = RealTimeStreamer(model)\n","\n","    print(f\"Processing your audio file: {file_path}\")\n","    results = streamer.process_uploaded_audio(file_path)\n","\n","    print(\"\\n=== FINAL TRANSCRIPTION ===\")\n","    full_text = \" \".join([result[1] for result in results])\n","    print(full_text)\n","\n","    return full_text\n","\n","if __name__ == \"__main__\":\n","    print(\"Ultra-Low Latency Real-Time Speech-to-Text System\")\n","    print(\"1. Train new model\")\n","    print(\"2. Demo with sample audio file\")\n","    print(\"3. Process your own audio file\")\n","    print(\"4. Simulate real-time processing\")\n","\n","    choice = input(\"Enter choice (1-4): \")\n","\n","    if choice == \"1\":\n","        model = train_model()\n","        print(\"Training completed!\")\n","    elif choice == \"2\":\n","        demo_file_transcription()\n","    elif choice == \"3\":\n","        file_path = input(\"Enter audio file path: \")\n","        process_your_audio_file(file_path)\n","    elif choice == \"4\":\n","        demo_realtime_simulation()\n","    else:\n","        print(\"Invalid choice\")\n","\n","# Jupyter notebook functions\n","def quick_train():\n","    return train_model()\n","\n","def quick_demo():\n","    return demo_file_transcription()\n","\n","def transcribe_file(file_path):\n","    return process_your_audio_file(file_path)"]},{"cell_type":"code","source":["import tarfile\n","file = tarfile.open('/content/drive/MyDrive/NAC/train-clean-100.tar.gz')\n","\n","file.extractall('/content/drive/MyDrive/NAC/train-clean-100')\n","file.close()"],"metadata":{"id":"9Hf4LiJZjNry","executionInfo":{"status":"ok","timestamp":1749420299516,"user_tz":-60,"elapsed":369285,"user":{"displayName":"Vishnu","userId":"03211991153580271542"}}},"execution_count":1,"outputs":[]}]}