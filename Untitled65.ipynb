{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V28","mount_file_id":"1bNwmC3NuYsYAsBJdNtJ6fIID-mcu3Is1","authorship_tag":"ABX9TyMCzfSzTNlmZ6XIlXXR476/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchaudio\n","import numpy as np\n","import librosa\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import OneCycleLR\n","import math\n","import string\n","import collections\n","import os\n","import threading\n","import queue\n","import time\n","from typing import Optional, Tuple, List\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Character vocabulary for LibriSpeech\n","VOCAB = [' ', \"'\"] + list(string.ascii_lowercase) + ['<blank>', '<sos>', '<eos>', '<pad>']\n","CHAR_TO_IDX = {char: idx for idx, char in enumerate(VOCAB)}\n","IDX_TO_CHAR = {idx: char for char, idx in CHAR_TO_IDX.items()}\n","VOCAB_SIZE = len(VOCAB)\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len).unsqueeze(1).float()\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n","                           -(math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe.unsqueeze(0))\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1)]\n","\n","class ConformerBlock(nn.Module):\n","    def __init__(self, d_model, n_heads, d_ff, kernel_size=31, dropout=0.1):\n","        super().__init__()\n","        self.d_model = d_model\n","\n","        # Multi-head self attention\n","        self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n","\n","        # Convolution module\n","        self.conv1 = nn.Conv1d(d_model, d_model * 2, 1)\n","        self.depthwise_conv = nn.Conv1d(d_model, d_model, kernel_size,\n","                                       padding=kernel_size//2, groups=d_model)\n","        self.conv2 = nn.Conv1d(d_model, d_model, 1)\n","\n","        # Feed forward\n","        self.ff1 = nn.Linear(d_model, d_ff)\n","        self.ff2 = nn.Linear(d_ff, d_model)\n","\n","        # Layer norms\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.norm3 = nn.LayerNorm(d_model)\n","        self.norm4 = nn.LayerNorm(d_model)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask=None):\n","        # Feed forward 1\n","        residual = x\n","        x = self.norm1(x)\n","        x = self.ff1(x)\n","        x = F.silu(x)\n","        x = self.dropout(x)\n","        x = self.ff2(x)\n","        x = self.dropout(x)\n","        x = residual + 0.5 * x\n","\n","        # Multi-head attention\n","        residual = x\n","        x = self.norm2(x)\n","        attn_out, _ = self.attention(x, x, x, key_padding_mask=mask)\n","        x = residual + attn_out\n","\n","        # Convolution module\n","        residual = x\n","        x = self.norm3(x)\n","        x = x.transpose(1, 2)  # (B, T, D) -> (B, D, T)\n","        x = self.conv1(x)\n","        x = F.glu(x, dim=1)\n","        x = self.depthwise_conv(x)\n","        x = F.silu(x)\n","        x = self.conv2(x)\n","        x = self.dropout(x)\n","        x = x.transpose(1, 2)  # (B, D, T) -> (B, T, D)\n","        x = residual + x\n","\n","        # Feed forward 2\n","        residual = x\n","        x = self.norm4(x)\n","        x = self.ff1(x)\n","        x = F.silu(x)\n","        x = self.dropout(x)\n","        x = self.ff2(x)\n","        x = self.dropout(x)\n","        x = residual + 0.5 * x\n","\n","        return x\n","\n","class StreamingConformer(nn.Module):\n","    def __init__(self, n_mels=80, d_model=512, n_layers=12, n_heads=8, d_ff=2048, vocab_size=VOCAB_SIZE, kernel_size=31, dropout=0.1):\n","        super().__init__()\n","\n","        # Feature extraction with depthwise separable convolutions\n","        self.feature_extractor = nn.Sequential(\n","            nn.Conv2d(1, 32, (3, 3), padding=(1, 1)),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 32, (3, 3), padding=(1, 1), groups=32),  # Depthwise\n","            nn.Conv2d(32, 64, 1),  # Pointwise\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d((2, 1)),  # Only pool in frequency\n","\n","            # Second conv block\n","            nn.Conv2d(64, 64, (3, 3), padding=(1, 1), groups=64),  # Depthwise\n","            nn.Conv2d(64, 128, 1),  # Pointwise\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d((2, 1)),  # Only pool in frequency\n","\n","            # Third conv block\n","            nn.Conv2d(128, 128, (3, 3), padding=(1, 1), groups=128),  # Depthwise\n","            nn.Conv2d(128, d_model, 1),  # Pointwise\n","            nn.BatchNorm2d(d_model),\n","            nn.ReLU(),\n","        )\n","\n","        # Calculate feature dimension after conv layers\n","        self.n_mels_after_conv = n_mels // 4  # After 2 pooling layers\n","\n","        # Project to model dimension\n","        self.input_projection = nn.Linear(d_model * self.n_mels_after_conv, d_model)\n","\n","        # Positional encoding\n","        self.pos_encoding = PositionalEncoding(d_model)\n","\n","        # Conformer blocks\n","        self.conformer_blocks = nn.ModuleList([\n","            ConformerBlock(d_model, n_heads, d_ff, kernel_size, dropout)\n","            for _ in range(n_layers)\n","        ])\n","\n","        # Output projection\n","        self.output_projection = nn.Linear(d_model, vocab_size)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, lengths=None):\n","        # x: (batch, n_mels, time)\n","        batch_size, n_mels, time_steps = x.shape\n","\n","        # Add channel dimension for conv2d\n","        x = x.unsqueeze(1)  # (batch, 1, n_mels, time)\n","\n","        # Feature extraction\n","        x = self.feature_extractor(x)  # (batch, d_model, n_mels_reduced, time)\n","\n","        # Reshape for transformer\n","        x = x.permute(0, 3, 1, 2)  # (batch, time, d_model, n_mels_reduced)\n","        x = x.reshape(batch_size, -1, self.n_mels_after_conv * x.size(2))\n","\n","        # Project to model dimension\n","        x = self.input_projection(x)  # (batch, time, d_model)\n","\n","        # Add positional encoding\n","        x = self.pos_encoding(x)\n","        x = self.dropout(x)\n","\n","        # Create attention mask if lengths provided\n","        mask = None\n","        if lengths is not None:\n","            max_len = x.size(1)\n","            mask = torch.arange(max_len, device=x.device).expand(\n","                batch_size, max_len) >= lengths.unsqueeze(1)\n","\n","        # Apply conformer blocks\n","        for block in self.conformer_blocks:\n","            x = block(x, mask)\n","\n","        # Output projection\n","        logits = self.output_projection(x)  # (batch, time, vocab_size)\n","\n","        return logits\n","\n","class LibriSpeechDataset(Dataset):\n","    def __init__(self, audio_paths, transcripts, sample_rate=16000, n_mels=80):\n","        self.audio_paths = audio_paths\n","        self.transcripts = transcripts\n","        self.sample_rate = sample_rate\n","        self.n_mels = n_mels\n","\n","        # Mel spectrogram transform\n","        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n","            sample_rate=sample_rate,\n","            n_mels=n_mels,\n","            n_fft=400,  # 25ms window\n","            hop_length=160,  # 10ms hop\n","            win_length=400,\n","            window_fn=torch.hann_window\n","        )\n","\n","    def __len__(self):\n","        return len(self.audio_paths)\n","\n","    def text_to_indices(self, text):\n","        # Convert text to character indices\n","        text = text.lower().strip()\n","        indices = []\n","        for char in text:\n","            if char in CHAR_TO_IDX:\n","                indices.append(CHAR_TO_IDX[char])\n","            else:\n","                indices.append(CHAR_TO_IDX[' '])  # Unknown char -> space\n","        return indices\n","\n","    def __getitem__(self, idx):\n","        # Load audio\n","        waveform, sr = torchaudio.load(self.audio_paths[idx])\n","\n","        # Resample if necessary\n","        if sr != self.sample_rate:\n","            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n","            waveform = resampler(waveform)\n","\n","        # Convert to mono\n","        if waveform.shape[0] > 1:\n","            waveform = waveform.mean(dim=0, keepdim=True)\n","\n","        # Extract mel spectrogram\n","        mel_spec = self.mel_transform(waveform)\n","        mel_spec = torch.log(mel_spec + 1e-8)  # Log mel\n","\n","        # Normalize\n","        mel_spec = (mel_spec - mel_spec.mean()) / (mel_spec.std() + 1e-8)\n","\n","        # Get transcript\n","        transcript = self.transcripts[idx]\n","        target_indices = self.text_to_indices(transcript)\n","\n","        return {\n","            'mel_spec': mel_spec.squeeze(0),  # Remove channel dim\n","            'target': torch.tensor(target_indices, dtype=torch.long),\n","            'input_length': mel_spec.shape[-1],\n","            'target_length': len(target_indices)\n","        }\n","\n","def load_custom_dataset(dataset_root=\"./dataset\"):\n","    \"\"\"\n","    Load custom dataset from Google Drive folder structure\n","\n","    Expected structure:\n","    dataset_root/\n","    ├── wavs/           # Contains .wav audio files\n","    ├── texts/          # Contains .txt transcript files with same filenames\n","    └── phonemes/       # Contains phoneme files (not used for now)\n","\n","    Args:\n","        dataset_root: Root directory of the dataset\n","\n","    Returns:\n","        audio_paths: List of paths to .wav files\n","        transcripts: List of corresponding transcriptions\n","    \"\"\"\n","    import glob\n","\n","    audio_paths = []\n","    transcripts = []\n","\n","    # Define paths\n","    wavs_dir = os.path.join(dataset_root, \"wavs\")\n","    texts_dir = os.path.join(dataset_root, \"texts\")\n","\n","    # Check if directories exist\n","    if not os.path.exists(wavs_dir):\n","        print(f\"Error: {wavs_dir} not found!\")\n","        return [], []\n","\n","    if not os.path.exists(texts_dir):\n","        print(f\"Error: {texts_dir} not found!\")\n","        return [], []\n","\n","    # Find all wav files\n","    wav_files = glob.glob(os.path.join(wavs_dir, \"*.wav\"))\n","\n","    print(f\"Found {len(wav_files)} wav files in {wavs_dir}\")\n","\n","    matched_files = 0\n","    missing_transcripts = []\n","\n","    for wav_file in wav_files:\n","        # Get base filename without extension\n","        base_name = os.path.splitext(os.path.basename(wav_file))[0]\n","\n","        # Construct corresponding text file path\n","        text_file = os.path.join(texts_dir, f\"{base_name}.txt\")\n","\n","        # Check if transcript exists\n","        if os.path.exists(text_file):\n","            try:\n","                # Read transcript\n","                with open(text_file, 'r', encoding='utf-8') as f:\n","                    transcript = f.read().strip()\n","\n","                # Skip empty transcripts\n","                if transcript:\n","                    audio_paths.append(wav_file)\n","                    transcripts.append(transcript)\n","                    matched_files += 1\n","                else:\n","                    print(f\"Warning: Empty transcript for {base_name}\")\n","\n","            except Exception as e:\n","                print(f\"Error reading {text_file}: {e}\")\n","        else:\n","            missing_transcripts.append(base_name)\n","\n","    # Report results\n","    print(f\"Successfully matched {matched_files} audio-transcript pairs\")\n","\n","    if missing_transcripts:\n","        print(f\"Warning: {len(missing_transcripts)} wav files have no corresponding transcripts:\")\n","        for missing in missing_transcripts[:10]:  # Show first 10\n","            print(f\"  - {missing}.txt\")\n","        if len(missing_transcripts) > 10:\n","            print(f\"  ... and {len(missing_transcripts) - 10} more\")\n","\n","    return audio_paths, transcripts\n","\n","def download_from_google_drive(drive_url, destination_folder=\"./dataset\"):\n","    \"\"\"\n","    Download dataset from Google Drive\n","\n","    Args:\n","        drive_url: Google Drive sharing URL\n","        destination_folder: Where to extract the dataset\n","    \"\"\"\n","    try:\n","        import gdown\n","        print(\"Using gdown to download from Google Drive...\")\n","\n","        # Create destination folder\n","        os.makedirs(destination_folder, exist_ok=True)\n","\n","        # Download and extract\n","        gdown.download_folder(drive_url, output=destination_folder, quiet=False)\n","        print(f\"Dataset downloaded to {destination_folder}\")\n","\n","    except ImportError:\n","        print(\"gdown not installed. Install with: pip install gdown\")\n","        print(\"Or manually download the dataset and place it in the expected structure:\")\n","        print(f\"{destination_folder}/\")\n","        print(\"├── wavs/\")\n","        print(\"├── texts/\")\n","        print(\"└── phonemes/\")\n","\n","    except Exception as e:\n","        print(f\"Download failed: {e}\")\n","        print(\"Please manually download and extract the dataset.\")\n","\n","def split_dataset(audio_paths, transcripts, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, random_seed=42):\n","    \"\"\"\n","    Split dataset into train/val/test sets\n","\n","    Args:\n","        audio_paths: List of audio file paths\n","        transcripts: List of transcripts\n","        train_ratio: Proportion for training\n","        val_ratio: Proportion for validation\n","        test_ratio: Proportion for testing\n","        random_seed: Random seed for reproducibility\n","\n","    Returns:\n","        Dictionary with train/val/test splits\n","    \"\"\"\n","    import random\n","\n","    # Set seed for reproducibility\n","    random.seed(random_seed)\n","\n","    # Create paired list and shuffle\n","    paired_data = list(zip(audio_paths, transcripts))\n","    random.shuffle(paired_data)\n","\n","    # Calculate split indices\n","    total_samples = len(paired_data)\n","    train_end = int(total_samples * train_ratio)\n","    val_end = train_end + int(total_samples * val_ratio)\n","\n","    # Split data\n","    train_data = paired_data[:train_end]\n","    val_data = paired_data[train_end:val_end]\n","    test_data = paired_data[val_end:]\n","\n","    # Separate paths and transcripts\n","    train_paths, train_transcripts = zip(*train_data) if train_data else ([], [])\n","    val_paths, val_transcripts = zip(*val_data) if val_data else ([], [])\n","    test_paths, test_transcripts = zip(*test_data) if test_data else ([], [])\n","\n","    print(f\"Dataset split:\")\n","    print(f\"  Train: {len(train_paths)} samples\")\n","    print(f\"  Validation: {len(val_paths)} samples\")\n","    print(f\"  Test: {len(test_paths)} samples\")\n","\n","    return {\n","        'train': (list(train_paths), list(train_transcripts)),\n","        'val': (list(val_paths), list(val_transcripts)),\n","        'test': (list(test_paths), list(test_transcripts))\n","    }\n","\n","def collate_fn(batch):\n","    # Sort by input length (descending)\n","    batch = sorted(batch, key=lambda x: x['input_length'], reverse=True)\n","\n","    # Get lengths\n","    input_lengths = torch.tensor([item['input_length'] for item in batch])\n","    target_lengths = torch.tensor([item['target_length'] for item in batch])\n","\n","    # Pad mel spectrograms\n","    max_input_len = max(input_lengths)\n","    n_mels = batch[0]['mel_spec'].shape[0]\n","\n","    padded_mels = torch.zeros(len(batch), n_mels, max_input_len)\n","    for i, item in enumerate(batch):\n","        length = item['input_length']\n","        padded_mels[i, :, :length] = item['mel_spec']\n","\n","    # Pad targets\n","    max_target_len = max(target_lengths)\n","    padded_targets = torch.full((len(batch), max_target_len),\n","                               CHAR_TO_IDX['<pad>'], dtype=torch.long)\n","    for i, item in enumerate(batch):\n","        length = item['target_length']\n","        padded_targets[i, :length] = item['target']\n","\n","    return {\n","        'mel_specs': padded_mels,\n","        'targets': padded_targets,\n","        'input_lengths': input_lengths,\n","        'target_lengths': target_lengths\n","    }\n","\n","class CTCLoss(nn.Module):\n","    def __init__(self, blank_idx=CHAR_TO_IDX['<blank>']):\n","        super().__init__()\n","        self.blank_idx = blank_idx\n","        self.ctc_loss = nn.CTCLoss(blank=blank_idx, reduction='mean', zero_infinity=True)\n","\n","    def forward(self, log_probs, targets, input_lengths, target_lengths):\n","        # log_probs: (batch, time, vocab)\n","        # targets: (batch, target_length)\n","        # input_lengths: (batch,)\n","        # target_lengths: (batch,)\n","\n","        # CTC expects (time, batch, vocab)\n","        log_probs = log_probs.transpose(0, 1)\n","        log_probs = F.log_softmax(log_probs, dim=-1)\n","\n","        # Flatten targets\n","        targets_flat = []\n","        for i, length in enumerate(target_lengths):\n","            targets_flat.extend(targets[i][:length].tolist())\n","        targets_flat = torch.tensor(targets_flat, dtype=torch.long, device=targets.device)\n","\n","        return self.ctc_loss(log_probs, targets_flat, input_lengths, target_lengths)\n","\n","class RealTimeSTT:\n","    def __init__(self, model_path=None, device='cuda' if torch.cuda.is_available() else 'cpu'):\n","        self.device = device\n","        self.model = StreamingConformer().to(device)\n","\n","        if model_path and os.path.exists(model_path):\n","            self.model.load_state_dict(torch.load(model_path, map_location=device))\n","\n","        self.model.eval()\n","\n","        # Audio parameters\n","        self.sample_rate = 16000\n","        self.chunk_duration = 0.03  # 30ms\n","        self.hop_duration = 0.01   # 10ms\n","        self.chunk_samples = int(self.chunk_duration * self.sample_rate)\n","        self.hop_samples = int(self.hop_duration * self.sample_rate)\n","\n","        # Mel transform\n","        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n","            sample_rate=self.sample_rate,\n","            n_mels=80,\n","            n_fft=400,\n","            hop_length=160,\n","            win_length=400,\n","            window_fn=torch.hann_window\n","        ).to(device)\n","\n","        # Streaming state\n","        self.audio_buffer = torch.zeros(0)\n","        self.context_frames = 50  # Keep some context\n","        self.mel_buffer = torch.zeros(80, 0)\n","\n","    def preprocess_audio(self, audio_chunk):\n","        \"\"\"Convert audio chunk to mel spectrogram\"\"\"\n","        if len(audio_chunk.shape) == 1:\n","            audio_chunk = audio_chunk.unsqueeze(0)\n","\n","        # Extract mel spectrogram\n","        mel_spec = self.mel_transform(audio_chunk)\n","        mel_spec = torch.log(mel_spec + 1e-8)\n","\n","        # Normalize\n","        mel_spec = (mel_spec - mel_spec.mean()) / (mel_spec.std() + 1e-8)\n","\n","        return mel_spec.squeeze(0)\n","\n","    def decode_predictions(self, logits):\n","        \"\"\"Decode CTC predictions to text\"\"\"\n","        # Get best path\n","        predictions = torch.argmax(logits, dim=-1)  # (time,)\n","\n","        # Remove blanks and consecutive duplicates\n","        decoded = []\n","        prev_char = None\n","\n","        for pred in predictions:\n","            pred_char = IDX_TO_CHAR[pred.item()]\n","            if pred_char != '<blank>' and pred_char != prev_char:\n","                decoded.append(pred_char)\n","            prev_char = pred_char\n","\n","        return ''.join(decoded)\n","\n","    def transcribe_stream(self, audio_chunk):\n","        \"\"\"Process a single audio chunk and return transcription\"\"\"\n","        with torch.no_grad():\n","            # Convert to tensor and move to device\n","            if isinstance(audio_chunk, np.ndarray):\n","                audio_chunk = torch.from_numpy(audio_chunk).float()\n","            audio_chunk = audio_chunk.to(self.device)\n","\n","            # Preprocess\n","            mel_spec = self.preprocess_audio(audio_chunk)\n","\n","            # Add to buffer with context\n","            self.mel_buffer = torch.cat([self.mel_buffer, mel_spec], dim=1)\n","\n","            # Keep only recent context\n","            if self.mel_buffer.shape[1] > self.context_frames:\n","                self.mel_buffer = self.mel_buffer[:, -self.context_frames:]\n","\n","            # Add batch dimension\n","            mel_batch = self.mel_buffer.unsqueeze(0)\n","\n","            # Forward pass\n","            logits = self.model(mel_batch)\n","\n","            # Decode\n","            text = self.decode_predictions(logits.squeeze(0))\n","\n","            return text\n","\n","def train_model(train_loader, val_loader, model, device, num_epochs=50):\n","    \"\"\"Training function\"\"\"\n","    model = model.to(device)\n","    criterion = CTCLoss()\n","    optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n","\n","    # Learning rate scheduler\n","    steps_per_epoch = len(train_loader)\n","    scheduler = OneCycleLR(optimizer, max_lr=1e-3,\n","                          steps_per_epoch=steps_per_epoch,\n","                          epochs=num_epochs)\n","\n","    best_val_loss = float('inf')\n","\n","    for epoch in range(num_epochs):\n","        # Training\n","        model.train()\n","        train_loss = 0\n","\n","        for batch_idx, batch in enumerate(train_loader):\n","            mel_specs = batch['mel_specs'].to(device)\n","            targets = batch['targets'].to(device)\n","            input_lengths = batch['input_lengths'].to(device)\n","            target_lengths = batch['target_lengths'].to(device)\n","\n","            optimizer.zero_grad()\n","\n","            # Forward pass\n","            logits = model(mel_specs, input_lengths)\n","\n","            # Calculate loss\n","            loss = criterion(logits, targets, input_lengths, target_lengths)\n","\n","            # Backward pass\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","            optimizer.step()\n","            scheduler.step()\n","\n","            train_loss += loss.item()\n","\n","            if batch_idx % 100 == 0:\n","                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0\n","\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                mel_specs = batch['mel_specs'].to(device)\n","                targets = batch['targets'].to(device)\n","                input_lengths = batch['input_lengths'].to(device)\n","                target_lengths = batch['target_lengths'].to(device)\n","\n","                logits = model(mel_specs, input_lengths)\n","                loss = criterion(logits, targets, input_lengths, target_lengths)\n","                val_loss += loss.item()\n","\n","        train_loss /= len(train_loader)\n","        val_loss /= len(val_loader)\n","\n","        print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n","\n","        # Save best model\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            torch.save(model.state_dict(), '/content/drive/MyDrive/NAC/best_conformer_model.pth')\n","            print(f'New best model saved with val loss: {val_loss:.4f}')\n","\n","# Example usage and demo\n","def simulate_real_time():\n","    \"\"\"Simulate real-time transcription\"\"\"\n","    # Initialize the real-time STT system\n","    stt = RealTimeSTT('best_conformer_model.pth')\n","\n","    # Simulate audio chunks (in practice, this would come from microphone)\n","    # For demo, we'll create some dummy audio\n","    sample_rate = 16000\n","    chunk_duration = 0.03  # 30ms\n","    chunk_samples = int(chunk_duration * sample_rate)\n","\n","    print(\"Simulating real-time transcription...\")\n","    print(\"Processing 30ms chunks with 10ms hop...\")\n","\n","    # Create dummy audio signal (in practice, replace with microphone input)\n","    duration = 2.0  # 2 seconds\n","    t = torch.linspace(0, duration, int(duration * sample_rate))\n","    # Mix of frequencies to simulate speech\n","    audio_signal = (torch.sin(2 * np.pi * 440 * t) * 0.3 +\n","                   torch.sin(2 * np.pi * 880 * t) * 0.2 +\n","                   torch.randn_like(t) * 0.1)\n","\n","    # Process in chunks\n","    hop_samples = int(0.01 * sample_rate)  # 10ms hop\n","    transcription = \"\"\n","\n","    for i in range(0, len(audio_signal) - chunk_samples, hop_samples):\n","        chunk = audio_signal[i:i + chunk_samples]\n","\n","        # Simulate processing time\n","        start_time = time.time()\n","\n","        # Transcribe chunk\n","        text = stt.transcribe_stream(chunk)\n","\n","        processing_time = (time.time() - start_time) * 1000  # ms\n","\n","        if text.strip():\n","            transcription += text + \" \"\n","            print(f\"Chunk {i//hop_samples}: '{text}' (processed in {processing_time:.1f}ms)\")\n","\n","    print(f\"\\nFinal transcription: {transcription.strip()}\")\n","\n","if __name__ == \"__main__\":\n","    # Example of how to use the system\n","    print(\"Real-Time Speech-to-Text System\")\n","    print(\"===============================\")\n","\n","    # Initialize model\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"Using device: {device}\")\n","\n","    model = StreamingConformer()\n","    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n","\n","    # For training, you would prepare your custom dataset:\n","    # audio_paths, transcripts = load_custom_dataset('./dataset')\n","    #\n","    # # Split into train/val/test\n","    # splits = split_dataset(audio_paths, transcripts)\n","    # train_paths, train_transcripts = splits['train']\n","    # val_paths, val_transcripts = splits['val']\n","    #\n","    # train_dataset = LibriSpeechDataset(train_paths, train_transcripts)\n","    # train_loader = DataLoader(train_dataset, batch_size=16,\n","    #                          shuffle=True, collate_fn=collate_fn)\n","    #\n","    # val_dataset = LibriSpeechDataset(val_paths, val_transcripts)\n","    # val_loader = DataLoader(val_dataset, batch_size=16,\n","    #                        shuffle=False, collate_fn=collate_fn)\n","    #\n","    #\n","\n","    audio_paths, transcripts = load_custom_dataset('/content/drive/MyDrive/NAC/')\n","\n","    # Split the data\n","    splits = split_dataset(audio_paths, transcripts)\n","    train_paths, train_transcripts = splits['train']\n","    val_paths, val_transcripts = splits['val']\n","    train_dataset = LibriSpeechDataset(train_paths, train_transcripts)\n","    train_loader = DataLoader(train_dataset, batch_size=16,\n","                              shuffle=True, collate_fn=collate_fn)\n","\n","    val_dataset = LibriSpeechDataset(val_paths, val_transcripts)\n","    val_loader = DataLoader(val_dataset, batch_size=16,\n","                            shuffle=False, collate_fn=collate_fn)\n","    train_model(train_loader, val_loader, model, device)\n","\n","    print(\"\\nSystem Features:\")\n","    print(\"- 30ms processing latency\")\n","    print(\"- 10ms hop length for real-time streaming\")\n","    print(\"- Conformer architecture with conv + transformer\")\n","    print(\"- CTC loss for alignment-free training\")\n","    print(\"- Optimized for LibriSpeech vocabulary\")\n","    print(\"- Streaming context management\")\n","    print(\"- GPU acceleration support\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YUw5rdMYEhdr","outputId":"f6edb40f-3434-4d92-b57c-b4f5be696e92"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Real-Time Speech-to-Text System\n","===============================\n","Using device: cpu\n","Model parameters: 52,845,728\n","Found 3842 wav files in /content/drive/MyDrive/NAC/wavs\n","Successfully matched 3842 audio-transcript pairs\n","Dataset split:\n","  Train: 3073 samples\n","  Validation: 384 samples\n","  Test: 385 samples\n","Epoch 0, Batch 0, Loss: 18.6699\n","Epoch 0, Batch 100, Loss: 3.2797\n","Epoch 0: Train Loss: 3.9070, Val Loss: 3.0994\n","New best model saved with val loss: 3.0994\n","Epoch 1, Batch 0, Loss: 3.1242\n","Epoch 1, Batch 100, Loss: 3.0362\n","Epoch 1: Train Loss: 3.0327, Val Loss: 3.0382\n","New best model saved with val loss: 3.0382\n","Epoch 2, Batch 0, Loss: 2.9482\n","Epoch 2, Batch 100, Loss: 2.9048\n","Epoch 2: Train Loss: 2.9051, Val Loss: 2.8439\n","New best model saved with val loss: 2.8439\n","Epoch 3, Batch 0, Loss: 2.7876\n","Epoch 3, Batch 100, Loss: 2.8716\n","Epoch 3: Train Loss: 2.8601, Val Loss: 2.8734\n","Epoch 4, Batch 0, Loss: 2.7504\n","Epoch 4, Batch 100, Loss: 2.7863\n","Epoch 4: Train Loss: 2.8355, Val Loss: 2.9200\n","Epoch 5, Batch 0, Loss: 2.8226\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"in4xJc9-Ehm_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"w_oZWbEREhp1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fMp9LWceEhtc"},"execution_count":null,"outputs":[]}]}